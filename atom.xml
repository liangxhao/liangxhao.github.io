<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>流裳照影</title>
  <subtitle>我的愿望是在白天走路</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://liangxhao.com/"/>
  <updated>2017-06-26T08:34:21.721Z</updated>
  <id>http://liangxhao.com/</id>
  
  <author>
    <name>liangxhao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Chapter 7: Multi-step Bootstrapping</title>
    <link href="http://liangxhao.com/2017/06/26/Chapter-7-Multi-step-Bootstrapping/"/>
    <id>http://liangxhao.com/2017/06/26/Chapter-7-Multi-step-Bootstrapping/</id>
    <published>2017-06-26T08:30:00.000Z</published>
    <updated>2017-06-26T08:34:21.721Z</updated>
    
    <content type="html"><![CDATA[<p>Reinforcement learning : an introduction(Second Edition). 读书笔记</p>
<p>本章将蒙特卡洛方法和单步时序差分算法统一起来给出了更为一般的方法。</p>
<a id="more"></a>
<p>前两章所讲的是时序差分学习方法（TD）和蒙特卡洛方法（MC），其中one-step TD方法是走一步就更新估计值，MC方法是走完所有的步才更新估计值，本章将one-step TD扩展到multi-step TD的情况，走n步后更新估计值。</p>
<p>one-step方法每次更新的时间间隔相同，实际中我们可能需要根据状态改变的缓慢来调整value更新频率，multi-step可以实现在更长的时间间隔内更新。</p>
<h4 id="n-step-td-预测">n-step TD 预测</h4>
<p>TD方法中的value在<span class="math inline">\(t\)</span>时刻之后经过<span class="math inline">\(n\)</span>步再更新的方法叫n-step TD。</p>
<p>考虑one-step TD和MC方法的backup diagram，可以扩展到<span class="math inline">\(n\)</span>步的情况，如下图：</p>
<div class="figure">
<img src="/images/rl_book/rl_7.1_0.jpg" alt="figure 1">
<p class="caption">figure 1</p>
</div>
<p>根据前两章one-step TD和MC的回报<span class="math inline">\(G_t\)</span>的形式，可以扩展写出<span class="math inline">\(n\)</span>步情况下的形式： <span class="math display">\[
G_{t}^{\left( n \right)}=R_{t+1}+\gamma R_{t+2}+\cdot \cdot \cdot +\gamma ^{n-1}R_{t+n}+\gamma ^nV_{t+n-1}\left( S_{t+n} \right)
\]</span> 其中<span class="math inline">\(n\ge 1,0\le t\le T-n\)</span>，当<span class="math inline">\(t+n\ge T\)</span>，即n-step的回报超过终止状态时，多余部分的reward记为0，也就是说<span class="math inline">\(G_{t}^{\left( n \right)}=G_t\)</span>。</p>
<p>此时就可以按照相同的方法立刻写出更新公式： <span class="math display">\[
V_{t+n}\left( S_t \right) =V_{t+n-1}\left( S_t \right) +\alpha \left[ G_{t}^{\left( n \right)}-V_{t+n-1}\left( S_t \right) \right]
\]</span> <em>error reduction property</em>，n-step的期望回报值小于<span class="math inline">\(\gamma^n\)</span>倍的<span class="math inline">\(V_{t+n-1}\)</span>，保证了<span class="math inline">\(V\)</span>在合适的条件下一定能收敛。</p>
<p>算法伪代码如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_7.1_1.jpg" alt="figure 2">
<p class="caption">figure 2</p>
</div>
<h4 id="n-step-sarsa">n-step Sarsa</h4>
<p>n-step方法也可以应用到控制问题中，例如Sarsa算法。</p>
<p>利用行为值写出n-step的回报值，如下： <span class="math display">\[
G_{t}^{\left( n \right)}=R_{t+1}+\gamma R_{t+2}+\cdot \cdot \cdot +\gamma ^{n-1}R_{t+n}+\gamma ^nQ_{t+n-1}\left( S_{t+n},A_{t+n} \right)
\]</span> 其更新公式自然可以写出： <span class="math display">\[
Q_{t+n}\left( S_t,A_t \right) =Q_{t+n-1}\left( S_t,A_t \right) +\alpha \left[ G_{t}^{\left( n \right)}-Q_{t+n-1}\left( S_t,A_t \right) \right]
\]</span> Expected Sarsa也可以推广过来，只需在最后<span class="math inline">\(G_{t}^{\left( n \right)}\)</span>最后一项<span class="math inline">\(Q\)</span>改为期望值即可，即： <span class="math display">\[
G_{t}^{\left( n \right)}=R_{t+1}+\gamma R_{t+2}+\cdot \cdot \cdot +\gamma ^{n-1}R_{t+n}+\gamma ^n\sum_a{\pi \left( a|S_{t+n} \right)}Q_{t+n-1}\left( S_{t+n},a \right) 
\]</span> 它们的backup diagram如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_7.2_0.jpg" alt="figure 3">
<p class="caption">figure 3</p>
</div>
<p>算法伪代码如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_7.2_1.jpg" alt="figure 4">
<p class="caption">figure 4</p>
</div>
<h4 id="重要性采样下的n-step-off-policy">重要性采样下的n-step Off-policy</h4>
<p>使用重要性采样的Off-policy方法也可以推广到n-step中。</p>
<p>其中重要性比率（importance sampling ratio）为如下形式： <span class="math display">\[
\rho _{t}^{t+n}=\prod_{k=t}^{\min \left( t+n-1,T \right)}{\frac{\pi \left( A_k|S_k \right)}{\mu \left( A_k|S_k \right)}}
\]</span> 相当于从<span class="math inline">\(t\)</span>到<span class="math inline">\(t+n-1\)</span>时刻的action都是采样而来的。</p>
<p>则可以简单的用<span class="math inline">\(\rho\)</span>作为权重，给出状态价值<span class="math inline">\(V\)</span>的更新公式如下： <span class="math display">\[
V_{t+n}\left( S_t \right) =V_{t+n-1}\left( S_t \right) +\alpha \rho _{t}^{t+n}\left[ G_{t}^{\left( n \right)}-V_{t+n-1}\left( S_t \right) \right] 
\]</span> 行为价值<span class="math inline">\(Q\)</span>的更新公式（即n-step Sarsa）如下： <span class="math display">\[
Q_{t+n}\left( S_t,A_t \right) =Q_{t+n-1}\left( S_t,A_t \right) +\alpha \rho _{t+1}^{t+n}\left[ G_{t}^{\left( n \right)}-Q_{t+n-1}\left( S_t,A_t \right) \right]
\]</span></p>
<p>注意这里的<span class="math inline">\(\rho\)</span>是从<span class="math inline">\(t+1\)</span>时刻开始。即<span class="math inline">\(t\)</span>时刻的action是我们所要计算的，而不是采样而来的。</p>
<p>对于n-step Expected Sarsa来说，在最后一步用行为价值的期望替换了采样值，所以这里的<span class="math inline">\(\rho _{t+1}^{t+n}\)</span>要改为<span class="math inline">\(\rho _{t+1}^{t+n-1}\)</span>。</p>
<p>算法的伪代码如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_7.3_0.jpg" alt="figure 5">
<p class="caption">figure 5</p>
</div>
<h4 id="不需要重要性采样的off-policy-n-step-tree-backup算法">不需要重要性采样的Off-policy: n-step Tree Backup算法</h4>
<p>Q-learning和Expected Sarsa都可以在one-step的情况下不需要重要性采样而实现Off-policy，对于n-step的情况该如何做呢？</p>
<p>这里给出一种tree-backup 的算法，如图是一个3-step backup diagram，这里以此为例进行说明。</p>
<div class="figure">
<img src="/images/rl_book/rl_7.4_0.jpg" alt="figure 6">
<p class="caption">figure 6</p>
</div>
<p>它是从一个action出发的state序列，每个state都要考虑它所有的action，策略<span class="math inline">\(\pi\)</span>和行为价值。其中被选中的那个action需要特殊考虑，因为产生了下一个状态的样本，所以它不需要bootstrap。</p>
<p>每个不被选中的action需要用目标策略<span class="math inline">\(\pi\)</span>的概率来加权它的行为价值，而那个被选中的action则需要对它产生的整个子树进行加权。</p>
<p>定义目标策略下的行为价值的期望值为： <span class="math display">\[
V_t=\sum_a{\pi \left( a|S_t \right)}Q_{t-1}\left( S_t,a \right)
\]</span> TD error定义为： <span class="math display">\[
\delta _t=R_{t+1}+\gamma V_{t+1}-Q_{t-1}\left( S_t,A_t \right)
\]</span> 则n-step的回报可写出如下公式：</p>
<div class="figure">
<img src="/images/rl_book/rl_7.4_1.jpg" alt="figure 7">
<p class="caption">figure 7</p>
</div>
<p>式子中，由于V中多加了被选中action的那部分，所以在随后被减掉。</p>
<p><span class="math inline">\(Q\)</span>的更新公式和n-step Sarsa的相同。</p>
<p>算法外代码如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_7.4_2.jpg" alt="figure 8">
<p class="caption">figure 8</p>
</div>
<h4 id="一个统一的算法n-step-qσ">一个统一的算法：n-step Q(σ)</h4>
<p>到目前为止，我们已经介绍了三种backup diagrams。n-step Sarsa算法的每一步都是采样转换，tree-backup 算法每一个分支都是状态-行为值转换，而n-step Expected Sarsa的最后一步是状态-行为转换，其它都是采样转换。</p>
<p>我们可以将这些情况统一起来，<span class="math inline">\(\sigma=0\)</span>表示纯的期望值（状态行为转换），<span class="math inline">\(\sigma=1\)</span>表示采样转换。如下图：</p>
<div class="figure">
<img src="/images/rl_book/rl_7.5_0.jpg" alt="figure 9">
<p class="caption">figure 9</p>
</div>
<p>其中<span class="math inline">\(\rho\)</span>表示在Off-policy的条件下，该处的转换需要重要性采样实现。</p>
<p>n-step Sarsa的回报可以写成依靠TD error的形式： <span class="math display">\[
G_{t}^{\left( n \right)}=Q_{t-1}\left( S_t,A_t \right) +\sum_{k=t}^{\min \left( t+n-1,T-1 \right)}{\gamma ^{k-t}}\left[ R_{k+1}+\gamma Q_k\left( S_{k+1},A_{k+1} \right) -Q_{k-1}\left( S_k,A_k \right) \right] 
\]</span> TD error可以写成带<span class="math inline">\(\sigma\)</span>的形式： <span class="math display">\[
\delta _t=R_{t+1}+\gamma \left[ \sigma _{t+1}Q_t\left( S_{t+1},A_{t+1} \right) +\left( 1-\sigma _{t+1} \right) V_{t+1} \right] -Q_{t-1}\left( S_t,A_t \right) 
\]</span> 由此可以定义n-step回报值：</p>
<div class="figure">
<img src="/images/rl_book/rl_7.5_1.jpg" alt="figure 10">
<p class="caption">figure 10</p>
</div>
<p>对于Off-policy，重要性采样比率被定义为： <span class="math display">\[
\rho _{t}^{t+n}=\prod_{k=t}^{\min \left( t+n-1,T-1 \right)}{\left( \sigma _k\frac{\pi \left( A_k|S_k \right)}{\mu \left( A_k|S_k \right)}+1-\sigma _k \right)}
\]</span> 算法伪代码如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_7.5_2.jpg" alt="figure 11">
<p class="caption">figure 11</p>
</div>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Reinforcement learning : an introduction(Second Edition). 读书笔记&lt;/p&gt;
&lt;p&gt;本章将蒙特卡洛方法和单步时序差分算法统一起来给出了更为一般的方法。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://liangxhao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement learning: an introduction" scheme="http://liangxhao.com/tags/Reinforcement-learning-an-introduction/"/>
    
      <category term="增强学习" scheme="http://liangxhao.com/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Chapter 6: Temporal-Difference Learning</title>
    <link href="http://liangxhao.com/2017/06/26/Chapter-6-Temporal-Difference-Learning/"/>
    <id>http://liangxhao.com/2017/06/26/Chapter-6-Temporal-Difference-Learning/</id>
    <published>2017-06-26T08:21:00.000Z</published>
    <updated>2017-06-26T09:42:08.806Z</updated>
    
    <content type="html"><![CDATA[<p>Reinforcement learning : an introduction(Second Edition). 读书笔记</p>
<p>本章是在动态规划方法和蒙特卡洛方法的基础上给出了一种更为实用的策略计算方法——时序差分算法。</p>
<a id="more"></a>
<p>时序差分算法（TD）是MC与DP的结合，MC必须到等到episode结束时才更新value，而TD可以在每步episode中更新；MC没有用到马尔可夫性，即在估计某个value时没有利用到其它的value，而TD因此具有bootstrap。</p>
<h4 id="td预测">TD预测</h4>
<p>根据上一章介绍的蒙特卡洛方法可知，一个简单的蒙特卡洛方法可以写成如下形式： <span class="math display">\[
V\left( S_t \right) \gets V\left( S_t \right) +\alpha \left[ G_t-V\left( S_t \right) \right]
\]</span> 其中<span class="math inline">\(\alpha\)</span>是一个固定的步长，这种方法记为<span class="math inline">\(constant-\alpha\)</span> MC。当一个episode结束时，<span class="math inline">\(G_t\)</span>才是可知的。</p>
<p>由于 <span class="math display">\[
v_{\pi}\left( s \right) =\mathbb{E}_{\pi}\left[ G_t|S_t=s \right] =\mathbb{E}_{\pi}\left[ R_{t+1}+\gamma v_{\pi}\left( S_{t+1} \right) |S_t=s \right]
\]</span> 所以MC可以与之结合，例如，TD(0)方法就是使用<span class="math inline">\(R_{t+1}+\gamma v_{\pi}\left( S_{t+1} \right)\)</span>代替<span class="math inline">\(G_t\)</span>，因而它具有bootstrap，就像DP算法那样，利用到了马尔科夫性，更新公式如下： <span class="math display">\[
V\left( S_t \right) \gets V\left( S_t \right) +\alpha \left[ R_{t+1}+\gamma V\left( S_{t+1} \right) -V\left( S_t \right) \right] 
\]</span> 算法流程如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_6.1_0.jpg" alt="figure 1">
<p class="caption">figure 1</p>
</div>
<p>TD error记为： <span class="math display">\[
\delta _t=R_{t+1}+\gamma V\left( S_{t+1} \right) -V\left( S_t \right) 
\]</span> 则MC的error经过计算可表示为： <span class="math display">\[
G_t-V\left( S_t \right) =\sum_{k=0}^{T-t-1}{\gamma ^k}\delta _{t+k}
\]</span> TD(0)的backup diagram如下图：</p>
<div class="figure">
<img src="/images/rl_book/rl_6.1_1.jpg" alt="figure 2">
<p class="caption">figure 2</p>
</div>
<h4 id="td预测方法的优点">TD预测方法的优点</h4>
<p>TD算法是由MC和DP结合而来的，所以具有它们的优点。</p>
<p>TD不需要环境模型（奖励reward、状态转移概率）</p>
<p>TD是即时的，即不需要等到episode完成时才能更新value</p>
<h4 id="td0的优化">TD(0)的优化</h4>
<p>如果episodes的数量比较有限，那么每个episode的每个step只能更新一次value，为了尽量利用资源，提高优化效率，可以采用批更新方法（<em>batch updating</em>）。</p>
<p>方法是：每次完成一个episode就将其保存下来，随之每次都要根据所有保存的episode更新value。例如完成第10个episode时，就已经保存了10个，然后用这10个再更新value。</p>
<p>certainty-equivalence estimate：只要给定正确的模型，我们就可以估计出正确的value。这等价于假设潜在的过程是已知的，而非是估计而来的。</p>
<p>batch TD(0)正属于这种certainty-equivalence estimate。</p>
<p>TD算法比MC表现要好，batch MC实际上是最小化训练集的均方根误差，而batch TD则是最大化MDP模型的最大似然值。</p>
<h4 id="sarsa-on-policy-td-control">Sarsa: On-Policy TD Control</h4>
<p>我们依旧根据GPI的模式解决控制问题，Sarsa是属于On-policy的算法。</p>
<p>episode的状态行为序列为：</p>
<div class="figure">
<img src="/images/rl_book/rl_6.4_0.jpg" alt="figure 3">
<p class="caption">figure 3</p>
</div>
<p>类似于预测问题，我们同样可以建立类似的更新方法： <span class="math display">\[
Q\left( S_t,A_t \right) \gets Q\left( S_t,A_t \right) +\alpha \left[ R_{t+1}+\gamma Q\left( S_{t+1},A_{t+1} \right) -Q\left( S_t,A_t \right) \right]
\]</span> 这里涉及到了五元组$( S_t,A_t,R_{t+1},S_{t+1},A_{t+1} ) $，其backup diagram如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_6.4_1.jpg" alt="figure 4">
<p class="caption">figure 4</p>
</div>
<p>伪代码形式的算法流程如下图：</p>
<div class="figure">
<img src="/images/rl_book/rl_6.4_2.jpg" alt="figure 5">
<p class="caption">figure 5</p>
</div>
<h4 id="q-learning-off-policy-td-control">Q-learning: Off-Policy TD Control</h4>
<p>Q-learning是属于Off-policy的算法，它与Sarsa稍有不同，其更新公式如下： <span class="math display">\[
Q\left( S_t,A_t \right) \gets Q\left( S_t,A_t \right) +\alpha \left[ R_{t+1}+\gamma \underset{a}{\max}Q\left( S_{t+1},a \right) -Q\left( S_t,A_t \right) \right]
\]</span> 伪代码形式的算法流程如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_6.5_0.jpg" alt="figure 6">
<p class="caption">figure 6</p>
</div>
<p>注意这里，off-policy与上面on-policy的不同：on-policy总是使用现有的（目标）策略（<span class="math inline">\(A\gets A&#39;\)</span>），而off-policy则是重新选择新的<span class="math inline">\(A\)</span>。</p>
<p>其backup diagram如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_6.5_1.jpg" alt="figure 7">
<p class="caption">figure 7</p>
</div>
<h4 id="expected-sarsa">Expected Sarsa</h4>
<p>在Q-learning中，用期望代替状态行为值的最大值，即为期望Sarsa算法。 <span class="math display">\[
\begin{align}
Q\left( S_t,A_t \right) &amp;\gets Q\left( S_t,A_t \right) +\alpha \left[ R_{t+1}+\gamma \mathbb{E}\left[ Q\left( S_{t+1},A_{t+1} \right) |S_{t+1} \right] -Q\left( S_t,A_t \right) \right] \\
&amp;\gets Q\left( S_t,A_t \right) +\alpha \left[ R_{t+1}+\gamma \sum_a{\pi \left( a|S_{t+1} \right)}Q\left( S_{t+1},a \right) -Q\left( S_t,A_t \right) \right] 
\end{align}
\]</span></p>
<p>其backup diagram如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_6.6_0.jpg" alt="figure 8">
<p class="caption">figure 8</p>
</div>
<p>这几种算法在性能上有所差异，原书进行了一定对比。</p>
<p>例如Expected Sarsa可以安全地设置<span class="math inline">\(\alpha\)</span>为1。而Sarsa则必须设置<span class="math inline">\(\alpha\)</span>较小，而且需要长期的优化。</p>
<h4 id="最大化偏差与两次学习">最大化偏差与两次学习</h4>
<p>无论是Q-learning还是Sarsa（<span class="math inline">\(\epsilon -greedy\)</span>）都有取最大值的操作，这会导致正的偏差。</p>
<p>例如真实值 $ q( s,a ) $ 是0，而它的估计值 $Q( s,a ) $ ，有时大于0，有时小于0，如果一直取最大值，那就会导致正的偏差。</p>
<p>一种解决方法是两次学习（Double Learning）。</p>
<ul>
<li><p>假设我们把数据集划分为两部分，用它们估计两组独立的参数：$Q_1( a ) $ 和 $Q_2( a ) $。</p></li>
<li><p>用<span class="math inline">\(Q_1\)</span>来找最大值的行为：$A^*=arg_a( Q_1( a ) ) $</p></li>
<li><p>用<span class="math inline">\(Q_2\)</span>来估计该行为的值：$Q_2( A^* ) =Q_2( arg_a( Q_1( a ) ) ) $</p></li>
<li><p>则可以得到无偏估计：<span class="math inline">\(\mathbb{E}\)</span>$[Q_2( A^* ) ] = q ( A^* ) $</p></li>
<li>同样地过程也可以反过来做，即先用<span class="math inline">\(Q_2\)</span>来找最大的行为，再用<span class="math inline">\(Q_1\)</span>来估计该行为的值。</li>
<li><p>每个step只做其中的一个</p></li>
</ul>
<p>这个思想是对MDPs的自然扩展，例如可以应用到Q-learning中去，其中<span class="math inline">\(Q_1\)</span>更新公式为： <span class="math display">\[
Q_1\left( S_t,A_t \right) \gets Q_1\left( S_t,A_t \right) +\alpha \left( R_{t+1}+\gamma Q_2\left( S_{t+1},\underset{a}{arg\max Q_1\left( S_{t+1},a \right) -Q_1\left( S_t,A_t \right)} \right) \right)
\]</span> <span class="math inline">\(Q_2\)</span>的公式也是类似的，每次更新时，以0.5的概率随机选择1或者2.</p>
<p>算法流程如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_6.7_0.jpg" alt="figure 9">
<p class="caption">figure 9</p>
</div>
<h4 id="games-afterstates-and-other-special-cases">Games, Afterstates, and Other Special Cases</h4>
<p>上面介绍的算法都属于一个统一的方式，例如它们都涉及到求行为值函数。</p>
<p>然而第一章展示的TD算法用于求解井字游戏中更像是状态值函数，但仔细一看，它并非通常意义下的状态值函数，也不是行为值函数。</p>
<p>传统的状态值函数估计的是那些有机会选择行为的状态，而井字游戏中的状态值函数是在agent做了移动后才估计，我们称之为afterstates，相应的值函数叫做afterstates值函数。</p>
<p>afterstates值函数对于我们对初始部分已知但是没必要完全知道的情况有用处，例如游戏中我们知道每走一步的奖励，我们知道象棋每一步可能移动到的地方，但是我们不知道对手怎么回应。afterstates很自然的利用到这些知识。</p>
<p>传统的行为值函数把位置和移动映射到一个估计值，但是很多位置-移动对会产生相同的位置结果，如下面的例子：</p>
<div class="figure">
<img src="/images/rl_book/rl_6.8_0.jpg" alt="figure 10">
<p class="caption">figure 10</p>
</div>
<p>afterstates会给出相等的value，但是传统的方法则不行。</p>
<p>afterstates在很多任务中十分普遍，这里不再细述，它仍然符合GPI框架。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Reinforcement learning : an introduction(Second Edition). 读书笔记&lt;/p&gt;
&lt;p&gt;本章是在动态规划方法和蒙特卡洛方法的基础上给出了一种更为实用的策略计算方法——时序差分算法。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://liangxhao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement learning: an introduction" scheme="http://liangxhao.com/tags/Reinforcement-learning-an-introduction/"/>
    
      <category term="增强学习" scheme="http://liangxhao.com/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Chapter 5: Monte Carlo Methods</title>
    <link href="http://liangxhao.com/2017/06/26/Chapter-5-Monte-Carlo-Methods/"/>
    <id>http://liangxhao.com/2017/06/26/Chapter-5-Monte-Carlo-Methods/</id>
    <published>2017-06-26T08:20:00.000Z</published>
    <updated>2017-06-26T09:47:08.820Z</updated>
    
    <content type="html"><![CDATA[<p>Reinforcement learning : an introduction(Second Edition). 读书笔记</p>
<p>本章介绍了在没有环境模型的条件下用来估计策略的蒙特卡洛方法。</p>
<a id="more"></a>
<p>上一章通过动态规划能够解决已知environment（即已知转移矩阵与reward函数）的MDP问题，根据是否已知policy可划分成prediction和control问题。</p>
<p>但是很多问题中environment是未知的，既不知道做了某个action之后会变到哪一个state，也不知道选择哪个action更好，也就是不知道environment的model是什么，在这种情况下需要解决的问题就是<strong>Model-free prediction</strong>和<strong>Model-free control</strong>。</p>
<p>显然这种新的问题只能从与environment的交互得到的experience中获取信息。</p>
<p>蒙特卡洛方法仅需要<strong>经验</strong>——与实际或者模拟环境进行交互的状态、行为和奖励的样本序列。</p>
<p>从<strong>实际环境</strong>中学习不需要事先了解环境的状态，仍然可以获得最优的行为。</p>
<p>从<strong>模拟环境</strong>中学习需要一个模型生成样本序列，然而上一章的DP算法则需要完整的概率分布。 在很多情况下，很容易根据概率分布来生成样本，而明确形式的概率分布却很难得到。</p>
<p>蒙特卡洛方法依靠样本回报的平均值来解决增强学习问题。</p>
<p>为了确保有明确的回报可用，这里定义蒙特卡洛方法仅用于情景性任务<strong>episode</strong>，情景完成时，价值估计和策略才会改变。</p>
<h4 id="蒙特卡洛预测">蒙特卡洛预测</h4>
<p>首先计算策略估计（prediction问题），即计算在给定策略下的状态-价值函数。</p>
<p>一个显然的方法是，计算该状态之后回报的平均值，随着观测次数 的增多，平均值应收敛于期望值，这是所有蒙特卡洛方法的基础。</p>
<p>假设我们估计的是 <span class="math inline">\(v_{\pi}\)</span>$ ( s ) $</p>
<p>在一个情景episode下，每次出现状态<span class="math inline">\(s​\)</span>，称为一个对<span class="math inline">\(s​\)</span>的访问（visit），当然在同一个情景下 <span class="math inline">\(s​\)</span> 可能会被访问多次，对<span class="math inline">\(s​\)</span>的第一次访问称为<em>first visit</em>。</p>
<ul>
<li><p><em>first-visit</em> MC：使用episodes中第一次对 <span class="math inline">\(s\)</span> 访问所得到的回报平均值来估计<span class="math inline">\(v_{\pi}\)</span>$( s ) $.</p></li>
<li><p><em>every-visit</em> MC：使用episodes中所有对 <span class="math inline">\(s\)</span> 访问后的回报平均值来估计<span class="math inline">\(v_{\pi}\)</span>$( s ) $.</p></li>
</ul>
<p>其中<em>every-visit</em>方法后面再讲，<em>first-visit</em> MC的算法流程如下图。</p>
<div class="figure">
<img src="/images/rl_book/rl_5.1_0.jpg" alt="figure 1">
<p class="caption">figure 1</p>
</div>
<p>例子：<strong>Blackjack</strong>游戏</p>
<p>与DP表达出所有可能的概率转换的backup diagram不同，蒙特卡洛的backup diagram只有情景任务的一个样本序列。episode需要执行到终止状态才能进行一次更新。</p>
<div class="figure">
<img src="/images/rl_book/rl_5.1_1.jpg" alt="figure 2">
<p class="caption">figure 2</p>
</div>
<h4 id="蒙特卡洛估计行为值">蒙特卡洛估计行为值</h4>
<p>除了状态-价值函数，还要估计行为-价值函数，这样才能从中选择出一个获得最大回报的行为。这和上一章DP算法中的做法是类似的。</p>
<p>假设我们估计的是 $q_{}( s,a ) $ ，所以这里需要考虑的是状态-行为对 <em>state–action pair</em> <span class="math inline">\((s,a)\)</span>。</p>
<p>同样存在两种方法：<em>first-visit</em> MC和<em>every-visit</em> MC</p>
<p>问题是，可能有很多状态-行为对没有被访问过。如果<span class="math inline">\(\pi\)</span>是一个确定性的策略，那么在每个状态下只有一个行为，其它的行为将不被访问到。这个问题属于<em>maintaining exploration</em>问题，可用如下方法解决。</p>
<p><em>exploring starts</em>方法：使情景以一个<em>state–action pair</em>开始。在情景开始时，每个<em>state–action pair</em>都有非零的概率被选择到。</p>
<p>当然上述方法不是一直有效的，尤其是在实际环境中，我们可能没法选择起始条件。</p>
<h4 id="蒙特卡洛控制">蒙特卡洛控制</h4>
<p>和上一章一样，这里广义策略迭代（generalized policy iteration）是解决这类最优策略问题的一般方法。这里不再赘述。</p>
<div class="figure">
<img src="/images/rl_book/rl_5.3_0.jpg" alt="figure 3">
<p class="caption">figure 3</p>
</div>
<p><span class="math display">\[
\pi_0 \xrightarrow{E} v_{\pi_0} \xrightarrow{I} \pi_1 \xrightarrow{E} v_{\pi_1} \xrightarrow{I} \pi_2 \xrightarrow{E} \cdots \xrightarrow{I} \pi_* \xrightarrow{E} v_*
\]</span></p>
<p>这与上一章DP算法类似，实现的过程中有两种方案。一种类似于策略迭代，在每轮的策略估计中都尽可能逼近<span class="math inline">\(q_{\pi _k}\)</span>，但这需要非常多的情景才能收敛到期望值；另一种类似于价值迭代，在策略估计未完成之时就就行策略提升，其中<em>in place</em>就是一种极端的方法。</p>
<p>ES版本的蒙特卡洛方法算法流程如下图：</p>
<div class="figure">
<img src="/images/rl_book/rl_5.3_1.jpg" alt="figure 4">
<p class="caption">figure 4</p>
</div>
<h4 id="不使用es的蒙特卡洛控制">不使用ES的蒙特卡洛控制</h4>
<p>为了去掉exploring starts这个实际中不现实的假设，唯一的方法就是确保每个action可以被无限次的选择到。</p>
<p>有两种可选的方法：<strong><em>on-policy</em></strong>、<strong><em>off-policy</em></strong></p>
<ul>
<li><em>on-policy</em>估计和提高策略所使用的数据来自于该策略本身</li>
<li><em>off-policy</em>估计和提高策略所使用的数据来自于其它策略</li>
</ul>
<p>其实带有ES假设的方法也属于on-policy，下面去掉该假设。</p>
<p><em>on-policy</em>控制方法一般都是柔性的（<strong><em>soft</em></strong>），意味着对于每个<span class="math inline">\(s\in \mathcal{S}\)</span> 以及 <span class="math inline">\(a\in \mathcal{A}\)</span><span class="math inline">\((s)\)</span> ，都有 <span class="math inline">\(\pi \left( a|s \right) &gt;0\)</span> 来解决这个问题。</p>
<p>例如第二章提到的<span class="math inline">\(\epsilon\)</span><em>-greedy</em>策略就是<em>soft</em>的一个特例，选择action时有<span class="math inline">\(\epsilon\)</span>的概率任意选择某个策略，而非总是选择当前最大回报的策略。这样在非贪婪时每个action被选择的最小概率是<span class="math inline">\(\frac{\epsilon}{\left| \mathcal{A}\left( s \right) \right|}\)</span>，其中贪婪的那个action被选中的概率是<span class="math inline">\(1-\epsilon +\frac{\epsilon}{\left| \mathcal{A}\left( s \right) \right|}\)</span>。下面是该算法的流程：</p>
<div class="figure">
<img src="/images/rl_book/rl_5.4_0.jpg" alt="figure 5">
<p class="caption">figure 5</p>
</div>
<p>该方法的简要证明过程见原书。</p>
<h4 id="重要性采样下的off-policy预测">重要性采样下的off-policy预测</h4>
<p>所有的控制学习方法都面临着同一个矛盾：它们的目标是最终选择最优的行为，但是为了探索哪些是最优的，又不能总是选择那些当前看起来是最优的。</p>
<p>上一节的on-policy采用<em>near-optimal policy</em>这个接近最优的方法来解决该问题，而off-policy则更直接的采用两个策略：<strong><em>behavior policy</em></strong>和<strong><em>target policy</em></strong></p>
<ul>
<li><p><em>behavior policy</em>：行为策略，用来生成数据，做出更多的探索</p></li>
<li><p><em>target policy</em>：目标策略，我们最终要学习的那个最优策略</p></li>
</ul>
<p>off-policy比on-policy复杂，具有更高的方差，优化比较慢，但是它更强大，更具有通用性。on-policy也可以看作是off-policy的行为策略和目标策略相同下的特殊情况。</p>
<p>这里用<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\pi\)</span>分别代表行为策略和目标策略。</p>
<p>为了使用<span class="math inline">\(\mu\)</span>来估计<span class="math inline">\(\pi\)</span>，必须要保证<span class="math inline">\(\pi\)</span>下选择的action，在<span class="math inline">\(\mu\)</span>下也会被选择到，即<span class="math inline">\(\pi \left( a|s \right) &gt;0\)</span>成立时<span class="math inline">\(\mu \left( a|s \right) &gt;0\)</span>也要成立，这称为优化假设。<span class="math inline">\(\mu\)</span>是随机的、更具有探索性的，而<span class="math inline">\(\pi\)</span>是确定具有贪婪性的策略。</p>
<p><em>off-policy</em>一般通过重要性采样（<em>importance sampling</em>）来实现，它是通过一个样本分布估计另一个样本分布期望值的方法。</p>
<p>产生状态-行为值的轨迹子序列（<em>subsequent state–action trajectory</em>）<span class="math inline">\(A_t,S_{t+1},A_{t+1},...,S_T\)</span>的策略<span class="math inline">\(\pi\)</span>的概率是 <span class="math display">\[
\prod_{k=t}^{T-1}{\pi \left( A_k|S_k \right)}p\left( S_{k+1}|S_k,A_k \right)
\]</span> 目标策略与行为策略的概率比值，即<em>importance-sampling ratio</em>为： <span class="math display">\[
\rho _{t}^{T}=\frac{\prod\limits_{k=t}^{T-1}{\pi \left( A_k|S_k \right) p\left( S_{k+1}|S_k,A_k \right)}}{\prod\limits_{k=t}^{T-1}{\mu \left( A_k|S_k \right) p\left( S_{k+1}|S_k,A_k \right)}}=\sum_{k=t}^{T-1}{\frac{\pi \left( A_k|S_k \right)}{\mu \left( A_k|S_k \right)}}
\]</span> 重要性采样这个比值作为回报的加权值来估计价值函数。它仅仅用到两个策略的比值，并没有完全用到MDP。</p>
<p>假设有很多episode样本。将它们按顺序接在一块，例如第一个场景在时刻100结束，那第二个场景就在时刻101开始。</p>
<p><span class="math inline">\(\mathfrak{J}\)</span>$ ( s ) $ 表示所有访问状态<span class="math inline">\(s\)</span>的时刻集合，$T( t ) $ 表示时刻<span class="math inline">\(t\)</span>的终止状态时刻，<span class="math inline">\(G_t\)</span> 表示从<span class="math inline">\(t\)</span>到 $T( t ) $ 的回报。</p>
<p>有两种估计价值的方法：</p>
<p>一种是一般性的方法<em>ordinary importance sampling</em>： <span class="math display">\[
V\left( s \right) =\frac{\sum_{t\in \mathcal{J}\left( s \right)}{\rho _{t}^{T\left( t \right)}G_t}}{\left| \mathcal{J}\left( s \right) \right|}
\]</span> 另一种带权值的方法是weighted importance sampling： <span class="math display">\[
V\left( s \right) =\frac{\sum_{t\in \mathcal{J}\left( s \right)}{\rho _{t}^{T\left( t \right)}G_t}}{\sum_{t\in \mathcal{J}\left( s \right)}{\rho _{t}^{T\left( t \right)}}}
\]</span> 一般性的方法是无偏估计，但是方差是无界的。带权值的方法具有低方差性，效果更好。</p>
<h4 id="增量实现">增量实现</h4>
<p>带权值的方法改写成如下形式： <span class="math display">\[
V_n=\frac{\sum\limits_{k=1}^{n-1}{W_k}G_k}{\sum\limits_{k=1}^{n-1}{W_k}}
\]</span> 则可以在每个场景结束后通过增量更新的方式实现： <span class="math display">\[
V_{n+1}=V_n+\frac{W_n}{C_n}\left[ G_n-V_n \right]
\]</span> 其中$C_{n+1}=C_n+W_{n+1} $</p>
<p>由此可以总结算法流程如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_5.6_0.jpg" alt="figure 6">
<p class="caption">figure 6</p>
</div>
<h4 id="off-policy蒙特卡洛控制">Off-Policy蒙特卡洛控制</h4>
<p>对于GPI这里不再赘述，直接给出流程，得到最优策略。</p>
<div class="figure">
<img src="/images/rl_book/rl_5.7_0.jpg" alt="figure 7">
<p class="caption">figure 7</p>
</div>
<h4 id="特定回报值的重要性采样">特定回报值的重要性采样</h4>
<p>上面所讲的方法仅仅对回报值进行加权处理，完全没有考虑回报值的内部结构，例如奖励的折扣率。</p>
<p>通过考虑这些问题可以减少off-policy估计的方差，这都还是当前比较前沿的研究。</p>
<p>例如episodes很长，而折扣 <span class="math inline">\(\gamma\)</span> 远小于1。具体来说场景有100个时刻之长，而 <span class="math inline">\(\gamma =0\)</span> ，那么 <span class="math inline">\(G_0=R_1\)</span> ，则 <span class="math inline">\(\rho\)</span> 的连乘形式中，只有第一项 <span class="math inline">\(\frac{\pi \left( A_0|S_0 \right)}{\mu \left( A_0|S_0 \right)}\)</span> 与回报值有关，其它因子是完全独立的，而且期望值为1。</p>
<p>解决这个问题的一种方法是使用<em>flat partial returns</em>来计算上述重要性采样公式中的<span class="math inline">\(G_t\)</span>。原理比较复杂，这里不再讨论。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Reinforcement learning : an introduction(Second Edition). 读书笔记&lt;/p&gt;
&lt;p&gt;本章介绍了在没有环境模型的条件下用来估计策略的蒙特卡洛方法。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://liangxhao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement learning: an introduction" scheme="http://liangxhao.com/tags/Reinforcement-learning-an-introduction/"/>
    
      <category term="增强学习" scheme="http://liangxhao.com/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Overcoming catastrophic forgetting in neural networks</title>
    <link href="http://liangxhao.com/2017/06/26/Overcoming-catastrophic-forgetting-in-neural-networks/"/>
    <id>http://liangxhao.com/2017/06/26/Overcoming-catastrophic-forgetting-in-neural-networks/</id>
    <published>2017-06-26T07:31:00.000Z</published>
    <updated>2017-06-26T11:01:55.209Z</updated>
    
    <content type="html"><![CDATA[<p>论文解析：Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., … Hadsell, R. (2016). Overcoming catastrophic forgetting in neural networks. arXiv Preprint. https://doi.org/10.1073/PNAS.1611835114</p>
<a id="more"></a>
<h4 id="问题来源">问题来源</h4>
<p>实现通用人工智能的关键步骤是获得连续学习的能力，即<em>agent</em>必须能在不遗忘旧任务的执行方法的同时习得如何执行新任务。</p>
<p>McCloskey 和 Cohen（1989）首先注意到，先训练一个神经网络学会给一个数字加 1，然后再训练该神经网络学会给数字加 2，但之后该网络就不会给数字加 1 了。他们将其称为<strong>灾难性遗忘</strong>（catastrophic forgetting）。</p>
<p>因为神经网络往往是通过快速覆写来学习新任务，而这样就会失去执行之前的任务所必需的参数。</p>
<p>研究表明，哺乳动物大脑的学习能力是通过突触后树突棘（postsynaptic dendritic spines）随时间而进行的形成和消除而实现的。尽管大多数新形成的树突棘后面都消失了，但仍有一小部分保留了下来，当特定的树突棘被擦除时，其对应的技能也会随之消失。</p>
<p>这些实验说明连续学习依赖于任务的突触巩固，知识通过一部分突触更少塑性而获得持久编码。</p>
<p>本文探讨了类似的办法，根据神经元的重要性来来改变单个神经元的可塑性，初步给出了解决问题的可能方法。</p>
<h4 id="推导演示">推导演示</h4>
<p>神经网络的学习就是调整权重以得到更好的输出。对于大型网络，许多不同的参数可以得到相似的结果，也就是神经网络的过参数化。</p>
<p>假设神经网络按顺序学习两个任务 A 和 B，参数记为<span class="math inline">\(\theta\)</span>，可以利用过参数化，使任务 B 的参数尽可能接近于任务 A。</p>
<div class="figure">
<img src="/images/papers/170411_offnn_0.jpg" alt="figure 1">
<p class="caption">figure 1</p>
</div>
<p><span class="math inline">\(\theta_A\)</span>代表在 A 任务中表现最好的 <span class="math inline">\(\theta\)</span> ，灰色区域表示的参数集合都可以接近于这个表现。</p>
<p>如果神经网络随后学习任务 B ，则将在蓝色箭头的方向上移动，获得任务B的参数区域。如果移动的过程中施加权重约束，可能得到绿色的移动方向，不仅不能很好的学习任务B，任务A的记忆也不能很好保留。</p>
<p>最好的办法是根据参数对任务的重要程度来选择其固化的程度，使得参数沿红色箭头移动，找到同时能够很好执行任务 A 和 B 的参数。</p>
<p>作者称这种方法为弹性权重巩固（Elastic Weight Consolidation）。</p>
<p>假设我们有一些数据 D，希望找到最可能的参数 <span class="math inline">\(\theta\)</span> ，其条件概率为： <span class="math display">\[
p\left( \theta |D \right) =\frac{p\left( D|\theta \right) p\left( \theta \right)}{p\left( D \right)}
\]</span> 写成对数似然的形式： <span class="math display">\[
\log p\left( \theta |D \right) =\log p\left( D|\theta \right) +\log p\left( \theta \right) -\log p\left( D \right)
\]</span> 假设D由两个独立部分构成，<span class="math inline">\(D_A\)</span>和<span class="math inline">\(D_B\)</span>分别是用于A、B任务的数据。则可得： <span class="math display">\[
p\left( D|\theta \right) =p\left( D_A|\theta \right) p\left( D_B|\theta \right) 
\]</span> 代入上一个式子，可得：</p>
<p><span class="math display">\[
\log p\left( \theta |D \right) =\log p\left( D_B|\theta \right) +\log p\left( \theta |D_A \right) -\log p\left( D_B \right)
\]</span></p>
<p>等式左边仍然表示的是整个数据集下计算参数的后验概率。</p>
<p>式右边仅仅取决于<span class="math inline">\(B\)</span>的损失函数，$p ( D_B|θ ) $ 。所有关于<span class="math inline">\(A\)</span>的信息都保留在后验概率 $p( θ|D_A ) $ ，它可以告诉我们哪些参数在解决A时很重要。</p>
<p>为了实现EWC，后验概率必须包含哪些参数对于<span class="math inline">\(A​\)</span>是重要的信息。真实的后验概率是不可知的，这里使用高斯分布进行估计，均值为<span class="math inline">\(\theta _{A}^{*}​\)</span>，方差由Fisher信息矩阵<span class="math inline">\(F​\)</span>给定。</p>
<p>$Fisher $信息是一种测量可观察随机变量 X 携带的关于 X 所依赖的概率的未知参数θ的信息的量的方法。我们需要测量来自 <span class="math inline">\(D_A\)</span>的每个数据所携带的关于<span class="math inline">\(\theta\)</span>的信息的量。</p>
<p><span class="math inline">\(F\)</span>的公式引自于另一篇论文，如下： <span class="math display">\[
\mathbf{F}_{\theta}=E\left[ \left( \nabla \log p_{\theta}\left( \mathbf{z} \right) \right) ^T\left( \nabla \log p_{\theta}\left( \boldsymbol{z} \right) \right) \right]
\]</span> <span class="math inline">\(F\)</span>满足三个属性：</p>
<ol style="list-style-type: decimal">
<li>它等价于损失函数接近于最小值的二阶导数；</li>
<li>它能通过一阶导数单独计算，对于非常大的模型也能简单计算；</li>
<li>它是半正定的；</li>
</ol>
<p>这种方法类似于期望传播，其中每个子任务都可被视为后验因素。</p>
<p>在这种估计下，学习任务A后，对于任务B的EWC最小化的损失函数<span class="math inline">\(L\)</span>是： <span class="math display">\[
L\left( \theta \right) =L_B\left( \theta \right) +\sum_i{\frac{\lambda}{2}}F_i\left( \theta _i-\theta _{A,i}^{*} \right) ^2
\]</span></p>
<p>其中$ L_B (θ) $ 是<span class="math inline">\(B\)</span>的损失，<span class="math inline">\(\lambda\)</span> 表示旧任务相对于新任务的重要性。</p>
<p>对于第三个任务C，EWC将尝试保持网络参数接近于A、B学习的参数。这通过两种方法实现，一是通过两个单独的惩罚项，二是两个二次惩罚之和作s为新的二次惩罚。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文解析：Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., … Hadsell, R. (2016). Overcoming catastrophic forgetting in neural networks. arXiv Preprint. https://doi.org/10.1073/PNAS.1611835114&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://liangxhao.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="神经网络" scheme="http://liangxhao.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Feedback Networks</title>
    <link href="http://liangxhao.com/2017/06/26/Feedback-Networks/"/>
    <id>http://liangxhao.com/2017/06/26/Feedback-Networks/</id>
    <published>2017-06-26T07:30:00.000Z</published>
    <updated>2017-06-26T10:38:42.413Z</updated>
    
    <content type="html"><![CDATA[<p>论文解析：Zamir, A. R., Wu, T.-L., Sun, L., Shen, W., Malik, J., &amp; Savarese, S. (2016). Feedback Networks. Retrieved from http://feedbacknet.stanford.edu/</p>
<a id="more"></a>
<h4 id="概要">概要</h4>
<p>目前在计算机视觉领域，最成功的学习模型一般都是学习连续的特征表示（successive representations），然后使用一个决策层（decision layer）进行分类或者识别。这种模型一般可以通过多层前馈网络实现，如ConvNets，其中的每一层都是连续特征表示的一部分。但是用反馈的方法也可以达到相同的效果，通过多次迭代，前一次迭代得到输出，反馈输入到模型中，得到下一次的输出，从而得到多个连续的特征表示。</p>
<p>反馈模型有以下优势：</p>
<ul>
<li>Early Predictions：可以得到早期的预测结果；</li>
<li>Taxnomoy Compliance：输出结果符合类别的分层结构；</li>
<li>Episodic Curriculum Learning：提供了新的理论基础；</li>
</ul>
<p>作者提出了基于反馈的一般学习模型，其测试结果比目前的前馈网络相当甚至更好，此外，作者还给出了并行化实现的方法，探究了反馈结构和模型设计的不同机制。</p>
<h4 id="详细内容">详细内容</h4>
<h5 id="优势">优势</h5>
<ol style="list-style-type: decimal">
<li>Early Predictions。</li>
</ol>
<p>反馈结构中只有当信号传到网络最后一层时才给出预测结果，而在反馈网络中可以在迭代中预测。如下图，第一次预测出是载具，第二次预测出是有轮载具，第三次预测出自行车，第四次预测出山地自行车。</p>
<div class="figure">
<img src="/images/papers/170315_fn_0.png" alt="figure 1">
<p class="caption">figure 1</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>Taxnomoy Compliance。</li>
</ol>
<p>上述每次的预测结果符合从粗到细的分类结构。</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Episodic Curriculum Learning。</p>
<p>Curriculum Learning在机器学习系统或者人类教育上都是一种重要的途径。在非凸优化问题上它帮助找到更好的最小值点，在优化问题上它可以加快优化速度。</p>
<p>前馈网络的预测结果是一次性的，所以要进行Curriculum Learning的话，必须要根据复杂度把训练数据赋予一个相同完整的网络，把不同的Curriculum安插在每一次测试当中（比如第一次训练给一些简单的例子，以后几次再给难一些的）。而反馈学习模型可以将Curriculum安插在一次测试中的任何一段，称为Episodic Curriculum Learning。</p></li>
</ol>
<h5 id="方法">方法</h5>
<p>作者给出了以循环神经网络（RNN）为基础的反馈模型的设计方法。</p>
<p>这里以ConvLSTM为例，完成进行图像分类的任务。</p>
<p>ConvLSTM就是用卷积操作代替LSTM的每一个元胞（Cell），LSTM的元胞结构如下，这里不再赘述。</p>
<div class="figure">
<img src="/images/papers/170315_fn_1.png" alt="figure 2">
<p class="caption">figure 2</p>
</div>
<p>反馈预测可通过ConvLSTM模型实现，具体如下：</p>
<ol style="list-style-type: decimal">
<li>在LSTM的每个时刻<span class="math inline">\(t\)</span>，输入为图像，用卷积代替<span class="math inline">\(cell\)</span>操作后，输出也为图像。增加图像一般的后处理过程（例如pooling, fully connected layer等），得到<span class="math inline">\(t\)</span>时刻的交叉熵损失<span class="math inline">\(L_t\)</span>，总的损失函数为各个时刻的损失之和，其中<span class="math inline">\(\gamma\)</span>是是折扣系数；</li>
</ol>
<p><span class="math display">\[
   L=\sum_{t=1}^T{\gamma ^tL_t},\ where\ L_t=-\log \frac{e^{H_{t}^{D}\left[ C \right]}}{\sum_j{e_{t}^{D}\left[ j \right]}}
   \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p>各个时刻的卷积参数<span class="math inline">\(W\)</span>是共享的，每个时刻都进行反向传播训练；</p></li>
<li><p>将网络的各个时刻展开如下图所示，其中红色箭头表示skip connections（类似于ResNet）</p></li>
</ol>
<div class="figure">
<img src="/images/papers/170315_fn_2.png" alt="figure 3">
<p class="caption">figure 3</p>
</div>
<ol start="4" style="list-style-type: decimal">
<li><p>作者提出的网络结构非常灵活，可以选择ConvLSTM模块中前馈网络的数量或者用不同数目的ConvLSTM模块进行堆叠。我们可以根据单个ConvLSTM模块中前馈层(Conv + BN)的数量对反馈网络进行分类。例如下图所示。分别记为Stack-1，Stack-2，Stack-All，其中Stack-i表示ConvLSTM中有<span class="math inline">\(i\)</span>个前馈层，这决定了隐藏状态在整个网络上的分布。</p>
<div class="figure">
<img src="/images/papers/170315_fn_3.png" alt="figure 4">
<p class="caption">figure 4</p>
</div></li>
</ol>
<p>网络的一些细节如下：</p>
<ol style="list-style-type: decimal">
<li><p><strong>Temporal Skip Connection</strong>。忽略连接，设<span class="math inline">\(t\)</span>表示第<span class="math inline">\(t\)</span>时刻，<span class="math inline">\(d\)</span>表示网络的深度（Physical Depth），则<span class="math inline">\(t\)</span>时刻，跳过的长度为<span class="math inline">\(n\)</span>，则新输入可表示为： <span class="math display">\[
  \widehat{X_{t}^{d}}=X_{t}^{d}+H_{t-n}^{d}
  \]</span></p></li>
<li><p><strong>Taxonomic Prediction</strong>。反馈网络系统可以在迭代过程中给出预测，早期是粗略的分类，随着迭代的进行，可以实现精细的分类。根据Softmax中的定义，识别对象属于某一类的概率： <span class="math display">\[
  P\left( y_i|x;W \right) =\frac{e^{f_{y_i}}}{\sum_j{e^{f_j}}}
  \]</span> 则它属于更高级的大类别的概率可以表示为其中各个小类的概率之和： <span class="math display">\[
P\left( Y_k|x;W \right) =\sum_{i\in 1:n}{P\left( y_i|x;W \right)}
\]</span></p></li>
<li><p><strong>Episodic Curriculum Learning</strong>。反馈网络在第<span class="math inline">\(k\)</span>次迭代过程中采用退火损失函数，则在第<span class="math inline">\(t\)</span>次粗分类与细分类损失函数可表示为：</p></li>
</ol>
<p><span class="math display">\[
   L\left( t \right) =\varsigma L_t^{Coarset}+\left( 1-\varsigma \right) L_t^{Fine}
   \]</span> 其中<span class="math inline">\(\varsigma =\frac{t}{k}\)</span>，<span class="math inline">\(t=0,1,2,...,k\)</span>，采用线性衰退的形式。</p>
<ol start="4" style="list-style-type: decimal">
<li><strong>Computation Graph Analysis</strong>。设网络<span class="math inline">\(m\)</span>为迭代次数，<span class="math inline">\(n\)</span>为物理深度。则前馈的深度为<span class="math inline">\(mn-1\)</span>，反馈的深度为<span class="math inline">\(m+n-1\)</span>，计算图如下，关于硬件并行方面的实现这里不细述。</li>
</ol>
<div class="figure">
<img src="/images/papers/170315_fn_4.png" alt="figure 5">
<p class="caption">figure 5</p>
</div>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文解析：Zamir, A. R., Wu, T.-L., Sun, L., Shen, W., Malik, J., &amp;amp; Savarese, S. (2016). Feedback Networks. Retrieved from http://feedbacknet.stanford.edu/&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://liangxhao.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="神经网络" scheme="http://liangxhao.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Chapter 4: Dynamic Programming</title>
    <link href="http://liangxhao.com/2017/04/18/Chapter-4-Dynamic-Programming/"/>
    <id>http://liangxhao.com/2017/04/18/Chapter-4-Dynamic-Programming/</id>
    <published>2017-04-18T13:23:08.000Z</published>
    <updated>2017-04-19T01:29:25.335Z</updated>
    
    <content type="html"><![CDATA[<p>Reinforcement learning : an introduction(Second Edition). 读书笔记</p>
<p>本章介绍了求解马尔科夫决策过程的基本算法：动态规划。总结出了解决增强学习问题的一般模型：广义策略迭代。</p>
<a id="more"></a>
<p>动态规划（DP）是假设环境完美满足MPD模型并计算其最优策略的一类算法。</p>
<p>由于经典的DP算法假设了完美的模型，而且计算量大，所以使用范围很有限，但是它为增强学习提供了必要的理论基础。</p>
<p>DP算法以及一般的增强学习问题的核心就是利用值函数构造搜索方式以得到最优策略。</p>
<p>最优值函数<span class="math inline">\(v_*\)</span>和<span class="math inline">\(q_*\)</span>满足最优贝尔曼等式：</p>
<p><span class="math display">\[
\begin{aligned}
    v_*\left( s \right) &amp;=\underset{a}{max}\,\,\mathbb{E}\left[ R_{t+1}+\gamma v_*\left( S_{t+1} \right) \,\,|\,\,S_t=s,A_t=a \right]\\
    &amp;=\underset{a}{max}\sum_{s&#39;,r}{p}\left( s&#39;,r|s,a \right) \left[ r+\gamma v_*\left( s&#39; \right) \right]\\ 
    q_*\left( s,a \right) &amp;=\mathbb{E}\left[ R_{t+1}+\gamma \underset{a&#39;}{\max}q_*\left( S_{t+1},a&#39; \right) \,\,|\,\,S_t=s,A_t=a \right]\\
    &amp;=\sum_{s&#39;,r}{p}\left( s&#39;,r|s,a \right) \left[ r+\gamma \underset{a&#39;}{\max}q_*\left( s&#39;,a&#39; \right) \right]\\
\end{aligned}
\]</span></p>
<h4 id="策略评估">策略评估</h4>
<p>对于任意给定的策略<span class="math inline">\(\pi\)</span>，计算其状态-值函数<span class="math inline">\(v_{\pi}\)</span>，称为策略评估（Policy Evaluation）。我们也称之为预测问题（prediction problem）。</p>
<p>下面这个是上一章给出的状态-值函数公式： <span class="math display">\[
\begin{aligned}
    v_{\pi}\left( s \right) &amp;\doteq \mathbb{E}_{\pi}\left[ \sum_{k=0}^{\infty}{\gamma}^kR_{t+k+1}|S_t=s \right]\\
    &amp;=\sum_a{\pi}\left( a|s \right) \sum_{s&#39;,r}{p}\left( s&#39;,r|s,a \right) \left[ r+\gamma v_{\pi}\left( s&#39; \right) \right] ,\,\,\forall s\in \mathcal{S}\\
\end{aligned}
\]</span></p>
<p>可以看出状态<span class="math inline">\(s\)</span>的值<span class="math inline">\(v_{\pi}\)</span>，是由其它状态<span class="math inline">\(s&#39;\)</span>决定的，如果环境是完全已知的，则上式可以通过线性方程直接求解： <span class="math display">\[
v_{\pi}=R^{\pi}+\gamma P^{\pi}v_{\pi}
\]</span> 实际情况下我们一般采用迭代方式求解。</p>
<p>从给定的初始值<span class="math inline">\(v_0\)</span>开始，估计一系列值<span class="math inline">\(v_0,v_1,v_2,...\)</span>，最终收敛到<span class="math inline">\(v_{\pi}\)</span>。下面是更新公式： <span class="math display">\[
\begin{aligned}
    v_{k+1}\left( s \right) &amp;=\mathbb{E}_{\pi}\left[ R_{t+1}+\gamma v_k\left( S_{t+1} \right) \,\,|\,\,S_t=s \right]\\
    &amp;=\sum_a{\pi}\left( a|s \right) \sum_{s&#39;,r}{p}\left( s&#39;,r|s,a \right) \left[ r+\gamma v_k\left( s&#39; \right) \right] ,\,\,\forall s\in \mathcal{S}\\
\end{aligned}
\]</span> 更新的停止条件一般是判断<span class="math inline">\(\max _{s\in S}\left| v_{k+1}\left( s \right) -v_k\left( s \right) \right|\)</span>是否足够小。这种方法被称为<em>迭代策略评估</em>（iterative policy evaluation）</p>
<p>对每个<span class="math inline">\(s\)</span>利用<span class="math inline">\(v_k\left( s \right)\)</span>来更新<span class="math inline">\(v_{k+1} \left( s \right)\)</span>，更新完所有的<span class="math inline">\(s\)</span>就是迭代一次，然后用<span class="math inline">\(v\)</span>新值取代旧值，进入下一次迭代，这种方法称为<em>full backup</em>，实现过程中使用两个数组，<span class="math inline">\(v_k\left( s \right)\)</span>作为旧值数组，<span class="math inline">\(v_{k+1} \left( s \right)\)</span>作为新值数组。</p>
<p>另一种更容易是<em>in place</em>，它与上述公式的原理有略微不同，有着更快的收敛速度。只使用一个数组，每次迭代就扫面（sweep）一遍状态<span class="math inline">\(s\)</span>，然后立刻更新<span class="math inline">\(v_k\left( s \right)\)</span>。</p>
<p>下面是<em>in place</em>版本的算法描述：</p>
<div class="figure">
<img src="/images/rl_book/rl_4.1_0.jpg" alt="figure 1">
<p class="caption">figure 1</p>
</div>
<h4 id="策略提升">策略提升</h4>
<p>我们计算值函数的目的是找到最好的策略。</p>
<p>前面计算的是某一任意策略<span class="math inline">\(\pi\)</span>下的值函数，我们想知道是在某些状态<span class="math inline">\(s\)</span>时，否应该改变这个策略，从中选出一个行动$a( s ) $，在此行动下可以得到更好的值（这称为Policy Improvement）。</p>
<p>从上一章可以知道行动-值函数的公式如下： <span class="math display">\[
\begin{aligned}
q_{\pi}\left( s,a \right) &amp;=\mathbb{E}_{\pi}\left[ R_{t+1}+\gamma v_{\pi}\left( S_{t+1} \right) \,\,|\,\,S_t=s,A_t=a \right] \\\
&amp;=\sum_{s&#39;,r}{p}\left( s&#39;,r|s,a \right) \left[ r+\gamma v_{\pi}\left( s&#39; \right) \right]
\end{aligned}
\]</span> 可以证明一定存在<span class="math inline">\(a=\pi&#39;\left(s\right)\)</span>，使得值函数取得最大，证明过程参见原文，称之为<em>policy improvement</em> <em>theorem</em>。</p>
<p>实际上就是贪婪策略（<em>greedy policy</em>），每次取<span class="math inline">\(a\)</span>使得值函数达到最大。 <span class="math display">\[
\begin{align}
\pi&#39;(s) &amp; = \underset{a}{argmax} \ q_{\pi}(s, a) \\
    &amp; = \underset{a}{argmax} \ \sum_{s&#39;, r} p(s&#39;,r|s,a) \left [ r + \gamma v_{\pi}(s&#39;) \right ] 
\end{align} 
\]</span></p>
<h4 id="策略迭代">策略迭代</h4>
<p>策略迭代（Policy Iteration）就是不断重复策略评估和策略提升的过程，以找到最优的策略。</p>
<p><strong>Generalized Policy Iteration</strong> 广义策略迭代 <span class="math display">\[
\pi_0 \xrightarrow{E} v_{\pi_0} \xrightarrow{I} \pi_1 \xrightarrow{E} v_{\pi_1} \xrightarrow{I} \pi_2 \xrightarrow{E} \cdots \xrightarrow{I} \pi_* \xrightarrow{E} v_*
\]</span> 动态规划的基本思想 - 通用策略迭代是：</p>
<ol style="list-style-type: decimal">
<li>从一个策略<span class="math inline">\(\pi_0\)</span>开始；</li>
<li>策略评估， 得到策略<span class="math inline">\(\pi_i\)</span>的价值<span class="math inline">\(v_{\pi _i}\)</span>；</li>
<li>策略提升， 根据价值价值<span class="math inline">\(v_{\pi _i}\)</span>，优化策略为<span class="math inline">\(v_{\pi _{i+1}}\)</span>；</li>
<li>迭代上面的步骤2和3，直到找到最优值<span class="math inline">\(v_*\)</span>，并得到最优策略<span class="math inline">\(\pi_*\)</span>；</li>
<li>终止条件：得到了稳定的策略<span class="math inline">\(\pi\)</span>和价值<span class="math inline">\(v_{\pi}\)</span>。</li>
</ol>
<p>算法流程描述如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_4.3_0.jpg" alt="figure 2">
<p class="caption">figure 2</p>
</div>
<h4 id="价值迭代">价值迭代</h4>
<p>策略迭代的收敛过程是比较缓慢的，必须等待策略评估完成后才能进行策略提升。</p>
<p>实际上策略评估在收敛之前每次迭代所得到的状态-值函数，其对应的贪婪策略可能就已经稳定不变了，所以我们基于此做出简化，可以优化到相同的结果。在策略评估过程中，对于每个状态<span class="math inline">\(s\)</span>，只找最优行动<span class="math inline">\(a\)</span>，称之为价值迭代（Value Iteration）。 <span class="math display">\[
\begin{aligned}
    v_{k+1}\left( s \right) &amp;\doteq \underset{a}{max}\,\,\mathbb{E}\left[ R_{t+1}+\gamma v_k\left( S_{t+1} \right) \,\,|\,\,S_t=s,A_t=a \right]\\
    &amp;=\underset{a}{max}\,\,\sum_{s&#39;,r}{p}\left( s&#39;,r|s,a \right) \left[ r+\gamma v_k\left( s&#39; \right) \right]\\
\end{aligned}
\]</span> 算法流程描述如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_4.4_0.jpg" alt="figure 3">
<p class="caption">figure 3</p>
</div>
<h4 id="异步动态规划">异步动态规划</h4>
<p>上面所讨论的算法计算量极其庞大。很多实际问题的状态空间，例如双陆琪游戏的状态空间可能高达<span class="math inline">\(10^{20}\)</span>，扫描一次可能需要上千年时间。</p>
<p>异步DP算法需要在每个并行单元保存所有状态值，可以非常灵活而且有效的减少计算量。</p>
<p>以下是三个简单的异步DP算法：</p>
<ul>
<li>In-place dynamic programming</li>
<li>Prioritised sweeping</li>
<li>Real-time dynamic programming</li>
</ul>
<h4 id="广义策略迭代">广义策略迭代</h4>
<p>广义策略迭代（generalized policy iteration，GPI）是策略评估与策略提升这两个过程交互的一般思想，这与过程的粒度等其它细节无关。几乎所有的增强学习问题都可以用GPI描述。</p>
<p>GPI可以被看作竞争与合作的关系。它们在相反的方向上竞争，贪婪的选择策略后会使值函数发生改变，值函数再进行调整以使值与策略相一致，再长期作用下，可以得到一个单一的联合解决方案：最优值函数和最优策略。</p>
<div class="figure">
<img src="/images/rl_book/rl_4.6_0.jpg" alt="figure 4">
<p class="caption">figure 4</p>
</div>
<p>策略评估与策略提升之间交互，为了共同的目标。</p>
<div class="figure">
<img src="/images/rl_book/rl_4.6_1.jpg" alt="figure 5">
<p class="caption">figure 5</p>
</div>
<p>下表是一个总结：</p>
<table style="width:100%;">
<colgroup>
<col width="13%">
<col width="51%">
<col width="35%">
</colgroup>
<thead>
<tr class="header">
<th align="center">Problem</th>
<th align="center">Bellman Equation</th>
<th align="center">Algorithm</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Prediction</td>
<td align="center">Bellman Expectation Equation</td>
<td align="center">Iterative Policy Evaluation</td>
</tr>
<tr class="even">
<td align="center">Control</td>
<td align="center">Bellman Expectation Equation+Greedy Policy Improvement</td>
<td align="center">Policy Iteration</td>
</tr>
<tr class="odd">
<td align="center">Control</td>
<td align="center">Bellman Optimality Equation</td>
<td align="center">Value Iteration</td>
</tr>
</tbody>
</table>
<h4 id="动态规划的效率">动态规划的效率</h4>
<p>DP对于非常大的问题可能并不实用，但与其他解决MDP的方法相比，DP方法实际上是很有效的。</p>
<p>DP的时间复杂度是关于状态数<span class="math inline">\(k\)</span>和动作数<span class="math inline">\(n\)</span>的多项式。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Reinforcement learning : an introduction(Second Edition). 读书笔记&lt;/p&gt;
&lt;p&gt;本章介绍了求解马尔科夫决策过程的基本算法：动态规划。总结出了解决增强学习问题的一般模型：广义策略迭代。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://liangxhao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement learning: an introduction" scheme="http://liangxhao.com/tags/Reinforcement-learning-an-introduction/"/>
    
      <category term="增强学习" scheme="http://liangxhao.com/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Chapter 3: Finite Markov Decision Processes</title>
    <link href="http://liangxhao.com/2017/04/18/Chapter-3-Finite-Markov-Decision-Processes/"/>
    <id>http://liangxhao.com/2017/04/18/Chapter-3-Finite-Markov-Decision-Processes/</id>
    <published>2017-04-18T13:21:45.000Z</published>
    <updated>2017-04-19T01:32:57.174Z</updated>
    
    <content type="html"><![CDATA[<p>Reinforcement learning : an introduction(Second Edition). 读书笔记</p>
<p>本章介绍了增强学习问题的基本模型，引入了马尔科夫决策过程和贝尔曼方程，这是以后解决增强学习问题的基础。</p>
<a id="more"></a>
<p>先建立描述增强学习问题的数学化形式，进行精确地理论陈述。</p>
<h4 id="代理-环境接口">代理-环境接口</h4>
<p>The Agent–Environment Interface.</p>
<p>代理（agent）：学习者或者决策者</p>
<p>环境（environment）：代理之外的一切，并与代理交互。</p>
<div class="figure">
<img src="/images/rl_book/rl_3.1_0.png" alt="figure 1">
<p class="caption">figure 1</p>
</div>
<p>代理与环境在离散时间序列中进行交互。在时刻<span class="math inline">\(t\)</span>，代理收到环境状态（state）的表达<span class="math inline">\(S_t\)</span>，<span class="math inline">\(S_t\in S\)</span>，<span class="math inline">\(S\)</span>是所有状态可能的集合。代理选择一个动作<span class="math inline">\(A_t\)</span>，<span class="math inline">\(A_t \in A\left( S_t \right)\)</span>，其中<span class="math inline">\(A\left( S_t \right)\)</span>是在状态<span class="math inline">\(S_t\)</span>下所有可用动作的集合。下一个时刻，代理将会收到将会收到一个数值的奖励<span class="math inline">\(R_t\)</span>，并且收到一个新的状态<span class="math inline">\(S_{t+1}\)</span>。</p>
<h4 id="情景性任务与连续性任务">情景性任务与连续性任务</h4>
<p>情节性任务(Episodic Tasks)，所有的任务可以被可以分解成一系列情节。逻辑上，可以看作为有限步骤的任务。</p>
<p>连续任务(Continuing Tasks) ，所有的任务不能分解。可以看作为无限步骤任务。</p>
<h4 id="目标与奖励">目标与奖励</h4>
<p>目标（goal）：代理的目标是最大化它在长时间内所收到的所有奖励之和。</p>
<p>设代理在时刻<span class="math inline">\(t\)</span>后收到的奖励分别为<span class="math inline">\(R_{t+1}\)</span>，<span class="math inline">\(R_{t+2}\)</span>，<span class="math inline">\(R_{t+3}\)</span>……</p>
<p>那么在<span class="math inline">\(t\)</span>时刻的目标是：</p>
<ol style="list-style-type: decimal">
<li>情节性任务</li>
</ol>
<p><span class="math display">\[
   G_t\doteq R_{t+1}+R_{t+2}+\cdot \cdot \cdot +R_T
   \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>连续性任务 <span class="math display">\[
   G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma ^2R_{t+3}\cdot \cdot \cdot =\sum_{k=0}^{\infty}{\gamma ^kR_{t+k+1}}
   \]</span> 其中<span class="math inline">\(0\le \gamma \le 1\)</span>称为折扣系数。</li>
</ol>
<p>为了简单，可以将这两种任务统一起来进行表达，如下图，可以认为情节性任务中代理进入终止状态以后，每个时刻收到的奖励都为0。</p>
<div class="figure">
<img src="/images/rl_book/rl_3.4_0.png" alt="figure 2">
<p class="caption">figure 2</p>
</div>
<p>统一之后的表达式如下： <span class="math display">\[
G_t\doteq \sum_{k=0}^{T-t-1}{\gamma ^kR_{t+k+1}}
\]</span> 其中$T=<span class="math inline">\(或者\)</span>=1$。</p>
<h4 id="马尔科夫属性">马尔科夫属性</h4>
<p>一般情况下环境在<span class="math inline">\(t+1\)</span>时刻的响应（状态<span class="math inline">\(s&#39;\)</span>、奖励<span class="math inline">\(r\)</span>）是由迄今为止所有的状态、行动和奖励决定的，可以用联合概率分布来表示： <span class="math display">\[
Pr\{S_{t+1} = s&#39;, R_{t+1} = r | S_0, A_0, R_1, S_1, A_1, \dots, R_t, S_t, A_t \}
\]</span></p>
<p>一个状态如果能够保留历史上所有相关的状态信号，则称它具有马尔科夫属性。</p>
<p>例如棋盘上所有棋子当前的状态，虽然之前很多序列的信息都丢失了，但是对于下一步真正重要的一切都有保留。</p>
<p>所以如果状态具有马尔科夫属性，那么上式就可以简化为如下形式： <span class="math display">\[
p(s&#39;, r | s, a) = Pr \{S_{t+1} = s&#39;, R_{t+1} = r | S_t = s, A_t = a \}
\]</span> 如果一个环境具有马尔科夫属性的话，在给定当前状态和行动后就可以预测下一个时刻的状态和奖励。通过迭代该式，就可以预测所有未来时刻的状态和奖励，甚至求出完整的历史信息。</p>
<h4 id="马尔科夫决策过程">马尔科夫决策过程</h4>
<p>一个增强学习任务如果满足马尔科夫属性，那么就称之为马尔科夫决策过程（MDP）。如果状态和动作空间是有限的，则称之为有限马尔科夫决策过程（finite MDP）。</p>
<p>在状态<span class="math inline">\(s\)</span>下，执行行动<span class="math inline">\(a\)</span>，转变为状态<span class="math inline">\(s&#39;\)</span>并获得奖赏<span class="math inline">\(r\)</span>的可能性： <span class="math display">\[
p(s&#39;, r | s, a) \doteq Pr \{S_{t+1} = s&#39;, R_{t+1} = r | S_t = s, A_t = a \}
\]</span> 在给定上式后就可以计算任何想知道的环境信息，例如：</p>
<p>state-action pairs，在状态<span class="math inline">\(s\)</span>下，执行动作<span class="math inline">\(a\)</span>的期望奖励： <span class="math display">\[
r(s,a) \doteq \mathbb{E}[R_{t+1} | S_t = s, A_t = a] = \sum_{r \in \mathcal{R}} r \sum_{s&#39; \in \mathcal{S}} p(s&#39;, r|s,a)
\]</span> state-transition probabilities，在状态<span class="math inline">\(s\)</span>下，执行行动<span class="math inline">\(a\)</span>，转变为状态<span class="math inline">\(s&#39;\)</span>的可能性： <span class="math display">\[
p\left( s&#39;|s,a \right) \doteq Pr\{S_{t+1}=s&#39;|S_t=s,A_t=a\}=\sum_{r\in \mathcal{R}}{p}\left( s&#39;,r|s,a \right)
\]</span></p>
<p>expected rewards for state–action–next-state triples，在状态<span class="math inline">\(s\)</span>下，执行行动<span class="math inline">\(a\)</span>，转变为状态<span class="math inline">\(s′\)</span>的期望奖赏： <span class="math display">\[
r\left( s,a,s&#39; \right) \doteq \mathbb{E}\left[ R_{t+1}|S_t=s,A_t=a,S_{t+1}=s&#39; \right] =\frac{\sum_{r\in \mathcal{R}}{r}p\left( s&#39;,r|s,a \right)}{p\left( s&#39;|s,a \right)}
\]</span> 例如一个回收机器人的例子。在每个时刻，机器人有三种选择：</p>
<p>（1）主动搜索罐头</p>
<p>（2）保持静止，等待有人带来罐头</p>
<p>（3）原路返回去充电</p>
<p>最好的方式是主动搜索罐头，但这会导致它耗尽电量而关机，而等待的过程不会耗电。如果机器人关机的话，那就会有人来重启它。</p>
<p>状态集合<span class="math inline">\(S=\{ high,low \}\)</span>，可能的行动有wait，search，recharge，则agent的行为集合是 <span class="math display">\[
\begin{align}
A \left( high \right) &amp;\doteq \{search,wait\} \\\
A \left( low \right) &amp;\doteq \{search,wait,recharge\} 
\end{align}
\]</span></p>
<p>具体信息参见原文，可以建立如下表格。</p>
<div class="figure">
<img src="/images/rl_book/rl_3.6_0.png" alt="figure 3">
<p class="caption">figure 3</p>
</div>
<p>转换图：</p>
<div class="figure">
<img src="/images/rl_book/rl_3.6_1.png" alt="figure 4">
<p class="caption">figure 4</p>
</div>
<h4 id="值函数">值函数</h4>
<p>策略<span class="math inline">\(\pi\)</span>是状态<span class="math inline">\(s\)</span>的映射，<span class="math inline">\(\pi \left( a| s \right)\)</span>是在状态<span class="math inline">\(s\)</span>下采取动作<span class="math inline">\(a\)</span>的概率。</p>
<p><strong>状态值函数</strong>，state-value function for policy <span class="math inline">\(\pi\)</span></p>
<p>在策略<span class="math inline">\(\pi\)</span>下状态<span class="math inline">\(s\)</span>的值函数，定义为<span class="math inline">\(v_{\pi} \left(s\right)\)</span>. <span class="math display">\[
v_{\pi}(s) \doteq \mathbb{E}[G_t | S_t = s] = \mathbb{E}_{\pi} \left [ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s \right ]
\]</span> <strong>行动值函数</strong>，action-value function for policy <span class="math inline">\(\pi\)</span></p>
<p>类似的定义，在策略<span class="math inline">\(\pi\)</span>状态<span class="math inline">\(s\)</span>采取行动<span class="math inline">\(a\)</span>的值函数，定义为<span class="math inline">\(q_{\pi} \left(s,a\right)\)</span>. <span class="math display">\[
q_{\pi}(s,a) \doteq \mathbb{E}[G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi} \left [ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a \right ] 
\]</span> 值函数满足递归性质，设<span class="math inline">\(s&#39;\)</span>表示下一时刻的状态，（迭代状态值函数）推导如下： <span class="math display">\[
\begin{align}
v_{\pi}(s) 
    &amp; \doteq \mathbb{E}[G_t | S_t = s] \\
    &amp; = \mathbb{E}_{\pi} \left [ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s \right ] \\
    &amp; = \mathbb{E}_{\pi} \left [ R_{t+1} + \gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+2} | S_t = s \right ] \\
    &amp; = \sum_{a} \pi(a|s) \sum_{s&#39;} \sum_{r} p(s&#39;,r|s,a) \left [ r + \gamma\mathbb{E}_{\pi} \left [ \sum_{k=0}^{\infty} \gamma^k R_{t+k+2} | S_{t+1} = s&#39; \right ] \right ] \\
    &amp; = \sum_{a} \pi(a|s) \sum_{s&#39;,r} p(s&#39;,r|s,a) \left [ r + \gamma v_{\pi}(s&#39;) \right ], \ \forall s \in \mathcal{S}
\end{align}
\]</span> 上式就是关于<span class="math inline">\(v_{\pi}\)</span>的<strong>Bellman equation</strong>。</p>
<p><strong>Backup diagrams</strong>，图形化可以表示如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_3.7_0.png" alt="figure 5">
<p class="caption">figure 5</p>
</div>
<h4 id="最优值函数">最优值函数</h4>
<p>解决增强学习问题，就是寻找策略以在长期内获得很多奖励。</p>
<p>对于有限MDP，可以精确的定义这个问题。存在一个最优的策略<span class="math inline">\(\pi_*\)</span>，它得到的value总是比其它策略更多。</p>
<p>在该策略下的值函数，称为最优值函数。</p>
<p><strong>最优状态值函数</strong>，optimal state-value function： <span class="math display">\[
v_*(s) \doteq \underset{\pi}{max} \ v_{\pi}(s), \forall s \in \mathcal{S}
\]</span> <strong>最优行动值函数</strong>，optimal action-value function： <span class="math display">\[
q_*(s) \doteq \underset{\pi}{max} \ q_{\pi}(s, a), \ \forall s \in \mathcal{S} \ and \ a \in \mathcal{A}(s)
\]</span> 最优行动值函数等于最优状态值函数下的最大期望： <span class="math display">\[
q_*(s,a) = \mathbb{E}[R_{t+1} + \gamma v_* (S_{t+1}) \ | \ S_t = s, A_t = a]
\]</span> <strong>Bellman Optimality equation</strong></p>
<p>同理，最优值函数也满足迭代性质。 <span class="math display">\[
\begin{align}
v_*(s)
    &amp; = \underset{a \in \mathcal{A}(s)}{max} \ q_{\pi_*}(s, a) \\
    &amp; = \underset{a}{max} \ \mathbb{E}_{\pi*} [G_t \ | \ S_t=s, A_t=a] \\
    &amp; = \underset{a}{max} \ \mathbb{E}_{\pi*} \left [ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \ | \ S_t=s, A_t=a \right ] \\
    &amp; = \underset{a}{max} \ \mathbb{E}_{\pi*} \left [ R_{t+1} + \gamma\sum_{k=0}^{\infty} \gamma^k R_{t+k+2} \ | \ S_t=s, A_t=a \right ] \\
    &amp; = \underset{a}{max} \ \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) \ | \ S_t=s, A_t=a ] \\
    &amp; = \underset{a \in \mathcal{A}(s)}{max} \sum_{s&#39;,r} p(s&#39;,r|s,a)[r + \gamma v_*(s&#39;)]  \\
\end{align}
\]</span> 上式就是关于<span class="math inline">\(v_*\)</span>的Bellman equation。</p>
<p>同样关于<span class="math inline">\(q_*\)</span>的Bellman equation如下： <span class="math display">\[
\begin{aligned}
    q_*\left( s,a \right) &amp;=\mathbb{E}\left[ R_{t+1}+\gamma \underset{a&#39;}{\max}q_*\left( S_{t+1},a&#39; \right) \,\,|\,\,S_t=s,A_t=a \right]\\
    &amp;=\sum_{s&#39;,r}{p}\left( s&#39;,r|s,a \right) \left[ r+\gamma \underset{a&#39;}{\max}q_*\left( s&#39;,a&#39; \right) \right]\\
\end{aligned}
\]</span></p>
<p><strong>Backup diagrams</strong>，图形化可以表示如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_3.7_1.png" alt="figure 6">
<p class="caption">figure 6</p>
</div>
<p>通过贝尔曼最优等式，可以解决回收机器人的问题：</p>
<div class="figure">
<img src="/images/rl_book/rl_3.7_2.png" alt="figure 7">
<p class="caption">figure 7</p>
</div>
<div class="figure">
<img src="/images/rl_book/rl_3.7_3.png" alt="figure 8">
<p class="caption">figure 8</p>
</div>
<h4 id="优化和近似">优化和近似</h4>
<p>得到最优策略需要极大的计算量和内存，需要进行优化和近似估计。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Reinforcement learning : an introduction(Second Edition). 读书笔记&lt;/p&gt;
&lt;p&gt;本章介绍了增强学习问题的基本模型，引入了马尔科夫决策过程和贝尔曼方程，这是以后解决增强学习问题的基础。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://liangxhao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement learning: an introduction" scheme="http://liangxhao.com/tags/Reinforcement-learning-an-introduction/"/>
    
      <category term="增强学习" scheme="http://liangxhao.com/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Chapter 2: Multi-arm Bandits</title>
    <link href="http://liangxhao.com/2017/04/18/Chapter-2-Multi-arm-Bandits/"/>
    <id>http://liangxhao.com/2017/04/18/Chapter-2-Multi-arm-Bandits/</id>
    <published>2017-04-18T13:20:00.000Z</published>
    <updated>2017-06-26T09:53:56.242Z</updated>
    
    <content type="html"><![CDATA[<p>Reinforcement learning : an introduction(Second Edition). 读书笔记</p>
<p>本章以多臂赌博机为例，介绍了增强学习所涉及的一些基本概念与学习方法。</p>
<a id="more"></a>
<p>增强学习能够从训练信息中评价所采取的行动，而不是以正确的行动作为指导，这是与其它类型学习的重要区别之一。</p>
<p>这两种方式分别称为评价性反馈和指导性反馈，评价性反馈取决于所采取的行动，指导性反馈与所采取的行动无关。</p>
<p>k-armed bandit问题是评价性反馈的一个简单版本，这里以此来介绍一些基本的学习方法。</p>
<h4 id="多臂赌博机问题">多臂赌博机问题</h4>
<p>假设有<span class="math inline">\(k\)</span>个老虎机并排放在我们面前，每一轮，我们可以选择一个老虎机来按，同时记录老虎机给出的奖励。假设各个老虎机不是完全相同的，经过多轮操作后，我们可以勘探出各个老虎机的部分统计信息,然后选择那个看起来奖励最高的老虎机。</p>
<h4 id="基本概念">基本概念</h4>
<p>在<span class="math inline">\(k\)</span>个选择（行动）中，每个选择都有一个期望奖励，这里称之为the value of action。</p>
<p>在第<span class="math inline">\(t\)</span>次行动中，我们的行动记为<span class="math inline">\(A_t\)</span>，其奖励记为<span class="math inline">\(R_t\)</span></p>
<p>在做了行动<span class="math inline">\(a\)</span>后，奖励的期望（true value）为$q_*( a ) $，即： <span class="math display">\[
q_*\left( a \right) =E\left[ R_t|A_t=a \right]
\]</span> 如果我们已经知道了选择每个老虎机的奖励，那么只需要选择奖励最高的那个老虎机即可。</p>
<p>但是在实际中奖励值是不可知的，需要通过多次实验进行估计。</p>
<p>在第<span class="math inline">\(t​\)</span>次行动，动作为 <span class="math inline">\(a​\)</span> 时，我们用 <span class="math inline">\(Q_t\left( a \right) ​\)</span> 来估计 <span class="math inline">\(q_*​\)</span><span class="math inline">\(\left( a \right) ​\)</span> ，即 <span class="math inline">\(Q_t\left( a \right)\)</span> <span class="math inline">\(\approx\)</span> <span class="math inline">\(q_*\left( a \right)\)</span></p>
<h4 id="行动-价值方法">行动-价值方法</h4>
<p>估计行动<span class="math inline">\(a\)</span>的奖励的期望，最自然的方法就是使用多次奖励的平均值。</p>
<p><span class="math display">\[
Q_t\left( a \right) =\frac{sum\ of\ reward\ when\ a\ taken\ prior\ to\ t}{number\ of\ times\ a\ taken\ prior\ to\ t}=\frac{\sum_{t=1}^{t-1}{R_i\cdot 1_{A_i=a}}}{\sum_{t=1}^{t-1}{1_{A_i=a}}}
\]</span> 其中 <span class="math display">\[
1_{A_i=a}=\left\{ \begin{matrix}
    1&amp;      if\ A_i=a\\
    0&amp;      otherwise\\
\end{matrix} \right. 
\]</span> 如果分母等于0，就设置 $Q_t( a ) $ 为默认值，例如 $Q_1( a )=0 $ 。当分母趋近于无穷大时，$Q_t( a ) $ 将会收敛于 $q_*( a ) $。</p>
<p>我们称这种方法为sample-average method，这只是比较简单的一种方法，不一定是最好的。</p>
<h4 id="ε-贪婪算法">ε-贪婪算法</h4>
<p>在第<span class="math inline">\(t\)</span>次行动，最简单的选择的方式，就是在奖励估计值中选择最大的那个，称之为贪婪方法（greedy） <span class="math display">\[
A_t\doteq \underset{a}{arg\max}Q_t\left( a \right)
\]</span> 贪婪算法总是选择使得当前取得最大化收益的行动，它并没有管其它行动以后是否会更好。</p>
<p>解决这个问题的办法是，以一个较小的概率<span class="math inline">\(\epsilon\)</span>，随机选择下一次的行动，而不是一直只选择奖励最大的那个。</p>
<p>这种方法的优点是，随着次数的增加，每个动作将会被抽样无穷大次，从而保证 $Q_t( a ) $ 收敛于 $q_*( a ) $。</p>
<p>假设<span class="math inline">\(k=10\)</span>，每个老虎机的奖励值从均值为$q_*( a ) $方差为1的高斯分布中选择，如下图所示，进行模拟测试。</p>
<div class="figure">
<img src="/images/rl_book/rl_2.2_1.png" alt="figure 1">
<p class="caption">figure 1</p>
</div>
<p>下图中比较了不同<span class="math inline">\(\epsilon\)</span>下得到的结果，贪婪算法在一开始比另外两个要快一些，但是随后就停滞不前。较大的<span class="math inline">\(\epsilon\)</span>能够较快地探索到到最优的动作，但是较小的<span class="math inline">\(\epsilon\)</span>在最后能得到更好的结果（图上未画出），所以可以考虑随着次数的动态的改变<span class="math inline">\(\epsilon\)</span>。<span class="math inline">\(\epsilon\)</span>的大小反映了探索与采用的权衡。</p>
<div class="figure">
<img src="/images/rl_book/rl_2.2_2.png" alt="figure 2">
<p class="caption">figure 2</p>
</div>
<p>ε-greedy算法与具体的任务相关，方差较大时需要更到的探索才能找到最优动作，而方差为0时，贪婪算法从不探索，在尝试一次后就知道奖励的真实值，所以实际上会表现的更好一些。如果该问题是非平稳的（这是经常遇到的），即随着时间的变化，奖励值或者最优动作发生变化，探索性的方法是必要的。</p>
<h4 id="增量实现">增量实现</h4>
<p>令<span class="math inline">\(R_i\)</span>表示第<span class="math inline">\(i\)</span>次选择某个行动。<span class="math inline">\(Q_n\)</span>表示选择该行动执行<span class="math inline">\(n-1\)</span>后估计的奖励期望值。 <span class="math display">\[
Q_n\doteq \frac{R_1+R_2+\cdot \cdot \cdot +R_{n-1}}{n-1}
\]</span> 将上式改写为如下形式： <span class="math display">\[
Q_{n+1}=Q_n+\frac{1}{n}\left[ R_n-Q_n \right]
\]</span> 与第一个公式相比，第二种方式的计算不需要存储最后一次行动之前的奖励值，节省了内存空间。</p>
<p>整个算法流程描述如下：</p>
<div class="figure">
<img src="/images/rl_book/rl_2.3_0.png" alt="figure 3">
<p class="caption">figure 3</p>
</div>
<p>类似的更新公式会频繁出现，这里表述为如下形式： <span class="math display">\[
NewEstimate←OldEstimate + StepSize  \left[ Target−OldEstimate \right]
\]</span> 其中<span class="math inline">\(\left[ Target−OldEstimate \right]\)</span>表示估计中的错误，朝着<span class="math inline">\(Target\)</span>的方向减少，<span class="math inline">\(Target\)</span>是假定的移动目标。例如本例中<span class="math inline">\(Target\)</span>是第<span class="math inline">\(n\)</span>次行动的奖励值。</p>
<p>步长（<span class="math inline">\(StepSize\)</span>)习惯上记为<span class="math inline">\(\alpha\)</span>，权值随着学习过程而变化时，记为 <span class="math inline">\(\alpha _t\)</span>$( a ) $ ，上述算法中取<span class="math inline">\(\alpha =\frac{1}{n}\)</span>。</p>
<h4 id="非稳定问题">非稳定问题</h4>
<p>在非平稳问题中，最近的奖励往往比长期的奖励更重要，实现的办法是采用恒定的步长参数。</p>
<p>将<span class="math inline">\(Q_{n+1}=Q_n+\alpha \left[ R_n-Q_n \right]\)</span>，$( 0,1 ] $ 改写为如下形式： <span class="math display">\[
Q_{n+1}=\left( 1-\alpha \right) ^nQ_1+\sum_{i=1}^n{\alpha \left( 1-\alpha \right)}^{n-i}R_i
\]</span></p>
<p>可以看出<span class="math inline">\(Q_{n+1}​\)</span>是<span class="math inline">\(Q_1​\)</span>与每个<span class="math inline">\(R_i​\)</span>的加权平均值。随着<span class="math inline">\(i​\)</span>的增大，<span class="math inline">\(R_i​\)</span>的权值以<span class="math inline">\(1-\alpha​\)</span>为指数增大。这有时被称为<span class="math inline">\(exponential, recency-weighted~average​\)</span>。</p>
<p>有时候步长参数逐渐发生变化是比较合适的，例如上述算法中的<span class="math inline">\(\alpha =\frac{1}{n}\)</span>，但并不是所有的<span class="math inline">\(\left\{ \alpha _n\left( a \right) \right\}\)</span> 都能满足要求。根据随机逼近理论，必须满足如下条件，才能保证估计值收敛于期望的概率逼近1。 <span class="math display">\[
\sum_{n=1}^{\infty}{a_n}\left( a \right) =\infty
\]</span></p>
<p><span class="math display">\[
\sum_{n=1}^{\infty}{a_n^2}\left( a \right) &lt;\infty
\]</span></p>
<p>第一个条件保证步长足够大以克服初始条件与随机因素的影响，第二个条件保证最终步长的尺寸足够小以收敛。</p>
<h4 id="优化初始值">优化初始值</h4>
<p>在上文的公式中可以看出，优化的初始值是<span class="math inline">\(Q_1 \left( a \right)\)</span>，需要由用户设定，在统计学中称为有偏见的初始估计。</p>
<p>在奖励求平均的方法中偏差会消失，但是其它方法则不一定，但是其权值<span class="math inline">\(\left( 1-\alpha \right) ^n\)</span>会随着抽样次数的增多而减小，偏差的影响就会越来越小。</p>
<p>假设实际的奖励期望为0，我们取初始值为5，可以鼓励行动-价值算法做出更多探索。</p>
<p>如下图所示，步长取恒定值0.1，贪婪算法取<span class="math inline">\(Q_1=5\)</span>，<span class="math inline">\(\epsilon\)</span>-贪婪算法取<span class="math inline">\(Q_1=0\)</span>。虽然贪婪算法一开始表现并不好，但是其最终结果比<span class="math inline">\(\epsilon\)</span>-贪婪算法要好很多。</p>
<div class="figure">
<img src="/images/rl_book/rl_2.5_0.png" alt="figure 4">
<p class="caption">figure 4</p>
</div>
<p>我们称这个技巧为<span class="math inline">\(optimistic~initial~values\)</span>方法，但是这种方法远远不是一种非常有用的方法，也不适用于非平稳问题，不需要太重视。</p>
<h4 id="置信上界选择算法">置信上界选择算法</h4>
<p>在<span class="math inline">\(\epsilon\)</span>-贪婪算法中，最好是根据行动的潜力来探索，而不是不加区别的选择。</p>
<p>置信上界选择算法（UCB）可理解为求每个动作的最大可信值，选择最大可信值最大的行动。 <span class="math display">\[
A_t\doteq \underset{a}{arg\max}\left[ Q_t\left( a \right) +c\sqrt{\frac{\log t}{N_t\left( a \right)}} \right]
\]</span> 其中<span class="math inline">\(c\)</span>是控制探索程度的参数，<span class="math inline">\(N_t\left( a \right)\)</span>表示行动<span class="math inline">\(a\)</span>被选择的次数。</p>
<p>平方根项表示的是<span class="math inline">\(a\)</span>值估计的方差或者不确定性，被最大化的量是<span class="math inline">\(a\)</span>的可能值的上界。每次选择动作<span class="math inline">\(a\)</span>时，<span class="math inline">\(a\)</span>的不确定性会减小；选择<span class="math inline">\(a\)</span>之外的动作时，<span class="math inline">\(a\)</span>的不确定性会增加。</p>
<p>实验结果如下图：</p>
<div class="figure">
<img src="/images/rl_book/rl_2.6_0.png" alt="figure 5">
<p class="caption">figure 5</p>
</div>
<h4 id="梯度算法">梯度算法</h4>
<p>上面的算法都是根据奖励的期望值来选择行动，它并非唯一的方法。</p>
<p>对于每个动作<span class="math inline">\(a\)</span>，我们定义倾向（preference）$H_t( a ) $，倾向性越大，该动作被选择的可能性越大。</p>
<p>利用soft-max函数定义选择动作<span class="math inline">\(a\)</span>的概率： <span class="math display">\[
Pr\left\{ A_t=a \right\} \doteq \frac{e^{H_t\left( a \right)}}{\sum_{b=1}^k{e^{H_t\left( b \right)}}}\doteq \pi _t\left( a \right)
\]</span> 初始化每个动作的倾向为0，然后每一步行动<span class="math inline">\(A_t\)</span>，利用随机梯度上升法更新<span class="math inline">\(H_t\left( a \right)\)</span>，依据下面的公式：</p>
<p><span class="math display">\[
\begin {aligned}
H_{t+1}\left( A_t \right)&amp;=H_t\left( A_t \right) +\alpha \left( R_t-\overline{R_t} \right) \left( 1-\pi _t\left( A_t \right) \right) \\
H_{t+1}\left( a \right) &amp;=H_t\left( a \right) -\alpha \left( R_t-\overline{R_t} \right) \pi _t\left( a \right) \ \ \ \forall a\ne A_t \\
\end {aligned}
\]</span></p>
<p>其中<span class="math inline">\(\overline{R_t}\)</span>是<span class="math inline">\(t\)</span>时间内所有奖励的均值，即<span class="math inline">\(\overline{R_t}=\frac{\sum{R_t}}{t}\)</span>，它代表一种基线（baseline），如果<span class="math inline">\(A_t\)</span>得到的奖励比基线高，那么以后选择该动作的概率会增大，否则便会减小。</p>
<p>该公式是由一般的梯度上升算法推导而来，如下式，具体推导过程参见原文。 <span class="math display">\[
H_{t+1}\left( a \right) =H_t\left( a \right) +\alpha \frac{\partial E\left[ R_t \right]}{\partial H_t\left( a \right)}
\]</span> 下图为实验结果，其中老虎机的期望奖励的高斯分布均值设为4，便于比较带基线于不带基线（<span class="math inline">\(\overline{R_t}=0\)</span>）的差别。</p>
<div class="figure">
<img src="/images/rl_book/rl_2.7.0.png" alt="figure 6">
<p class="caption">figure 6</p>
</div>
<h4 id="关联性搜索">关联性搜索</h4>
<p>假设在学习任务中，每一步的最佳动作随机发生变化，而上述非平稳的方法只适应于变化缓慢的情况。这就需要其它的线索来解决这个问题，除了奖励值以外可以考虑其它相关信息，例如老虎机会发出颜色上的信号，我们可以将其与学习任务关联起来。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Reinforcement learning : an introduction(Second Edition). 读书笔记&lt;/p&gt;
&lt;p&gt;本章以多臂赌博机为例，介绍了增强学习所涉及的一些基本概念与学习方法。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://liangxhao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement learning: an introduction" scheme="http://liangxhao.com/tags/Reinforcement-learning-an-introduction/"/>
    
      <category term="增强学习" scheme="http://liangxhao.com/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Chapter 1: The Reinforcement Learning Problem</title>
    <link href="http://liangxhao.com/2017/04/18/Chapter-1-The-Reinforcement-Learning-Problem/"/>
    <id>http://liangxhao.com/2017/04/18/Chapter-1-The-Reinforcement-Learning-Problem/</id>
    <published>2017-04-18T13:16:54.000Z</published>
    <updated>2017-04-19T01:27:48.948Z</updated>
    
    <content type="html"><![CDATA[<p>Reinforcement learning : an introduction(Second Edition). 读书笔记</p>
<p>本章主要对增强学习的基本问题做了一个简要的介绍。给出了一些例子，以说明其特点与发展历史。</p>
<a id="more"></a>
<p>与环境互动学习是我们知识的主要来源。</p>
<p>这种以目标为导向与不确定的环境进行交互的学习方法，我们称之为增强学习。</p>
<p>与监督学习和非监督学习不同：</p>
<ul>
<li>监督学习是通过有标签的数据，学习泛化的能力；</li>
<li>非监督学习是通过无标签的数据，找到隐藏的模式。</li>
</ul>
<h4 id="特点">特点</h4>
<ul>
<li><p>exploration和exploitation之间的权衡：利用已发现的有效行动、探索未发现的可能更好的行动</p></li>
<li><p>考虑整个问题而不是子问题</p></li>
<li><p>接近于动物的心理学</p></li>
<li><p>实现通用人工智能的一部分</p></li>
</ul>
<h4 id="例子">例子</h4>
<p>依据与环境交互做出的反馈积累经验，逐渐提高其行为能力</p>
<ul>
<li>下棋</li>
<li>搅拌控制器在石油提炼操作中逐步优化产量、成本、质量之间的平衡，而不是严格按照工程师最初的设计来操作</li>
<li>机器人进入房间寻找垃圾，寻找一条回去充电的路</li>
<li>小羚羊在出生时挣扎地站起来，然后半小时后每小时能跑几十公里</li>
<li>人类的探索着完成某一项任务的流程方案</li>
</ul>
<h4 id="组成元素">组成元素</h4>
<ul>
<li><p>policy：策略，从感知的环境状态到采取的行为动作的映射，即agent在特定时间下的行为方式</p></li>
<li><p>reward signal：奖励信号，增强学习问题的目标，agent可以改变其策略以获得更高的奖励</p></li>
<li><p>value function：值函数，从这个状态开始，期望在未来获得的奖励总额</p></li>
<li><p>model of environment：环境模型，模拟环境的行为，即在给的动作和状态下，预测下一个状态和奖励</p></li>
</ul>
<h4 id="范围和限制">范围和限制</h4>
<p>并非所有增强方法都需要值函数。例如遗传算法、基因编程和模拟退火等进化算法十分接近于增强学习，在涉及与环境交互作用的时候，它们忽略了许多问题本身的结构，不关注经历过的状态和采取过的动作，但是当agent不能准确获取环境状态时，这些进化算法又有一定优势。</p>
<p>与优化方法不同，增强学习的目标是最大化奖励，而不是实现优化问题上的最大化。增强学习也会遇到一些不可预料的行为状态，以意想不到的方式获得奖励，换个角度说这属于创造力。</p>
<h4 id="井字游戏">井字游戏</h4>
<p>在3x3的棋盘上，某个玩家如果能优先在水平、垂直或者对角上放三个相同的标志即可获胜，类似于下图中的状态属于平局。一个熟练的玩家可以做到从不失手，我们这里假设对手并不完美，能够允许我们赢，在此讨论该问题。</p>
<div class="figure">
<img src="/images/rl_book/rl_1.5_0.png" alt="figure 1">
<p class="caption">figure 1</p>
</div>
<p>经典方法并不能给出令人满意的解决方法，例如游戏理论中的极大极小值（minimax）算法假设了对手特定的下手方式；动态规划的方法又需要输入对手完整的说明，如在每个状态中移动的概率。我们假设这些先验信息是不可用的，需要先学习一个对手行为的模型，再利用动态规划求解。</p>
<p>假设自己执O，对手执X，定义值函数（value function）为赢的概率，学习过程描述如下：</p>
<ol style="list-style-type: decimal">
<li><p>首先构建一个哈希表，代表棋盘上所有可能的状态；</p></li>
<li><p>初始化每个状态对应的值函数，其中，赢的状态初始化为1，其它状态初始化为0.5，表示有0.5的概率会赢；</p></li>
<li><p>先手和后手依据依据设定的概率选择下面规则之一，轮流执棋，直到该次比赛结束，并保留从exploitation到比赛结束所有的状态序列：</p></li>
</ol>
<ul>
<li>exploration：依据值函数的最大值，确定下一步状态；</li>
<li>exploitation：随机选择下一状态；</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li><p>如果赢得比赛，奖励（reward）为1，平局或者失败，奖励为0.</p></li>
<li><p>依据下式，在状态序列中倒序更新值函数，其中<span class="math inline">\(s&#39;\)</span>是<span class="math inline">\(s\)</span>的下一个状态，<span class="math inline">\(\alpha\)</span>为更新的步长，首次更新时（<span class="math inline">\(s\)</span>没有下一状态）取<span class="math inline">\(V\left(s&#39; \right)\)</span>为reward；</p></li>
</ol>
<p><span class="math display">\[
   V\left( s \right) =V\left( s \right) +\alpha \left[ V\left( s&#39; \right)- V\left( s \right) \right]
   \]</span></p>
<ol start="6" style="list-style-type: decimal">
<li><p>进行多次比赛，重复3-5步操作，每次都采用上次的更新过的值函数；</p></li>
<li><p>一定次数之后完成学习过程。</p></li>
</ol>
<p>上述学习过程被称为时间差分学习法（temporal-difference learning method），可用下图描述</p>
<div class="figure">
<img src="/images/rl_book/rl_1.5_1.png" alt="figure 2">
<p class="caption">figure 2</p>
</div>
<p>其中实线表示实际选择的状态，虚线表示经过考虑但是没有选择的状态。<span class="math inline">\(e^*\)</span>是当时值函数最高的状态，但是我们选择了<span class="math inline">\(e\)</span>进行探索。箭头指向的是需要保存的状态序列的初始点。</p>
<p>学习完成之后，将状态和值函数表保存起来即可。对弈的时候，通过查找该表实现。</p>
<h4 id="历史">历史</h4>
<p>两条主线：</p>
<ul>
<li>动物学习心理学的试错法(trial-and-error)。</li>
<li>优化控制(optimal control) ，动态编程(dynamic programming)，时间差分(temporal difference)。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Reinforcement learning : an introduction(Second Edition). 读书笔记&lt;/p&gt;
&lt;p&gt;本章主要对增强学习的基本问题做了一个简要的介绍。给出了一些例子，以说明其特点与发展历史。&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://liangxhao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement learning: an introduction" scheme="http://liangxhao.com/tags/Reinforcement-learning-an-introduction/"/>
    
      <category term="增强学习" scheme="http://liangxhao.com/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
